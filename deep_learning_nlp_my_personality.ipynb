{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep learning mypersonality_tcc",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kIhY6Nsn_jhnl5l78f7kokGbYiyzuzh0",
      "authorship_tag": "ABX9TyM1rRW37b8aHi2K2Kqgbp++",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagomotax/nlp-user-profiles/blob/master/deep_learning_nlp_my_personality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fYFX0edwixu",
        "outputId": "539ed1da-a570-4c26-e971-7e428816d2f8"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z21PAHJAgALN"
      },
      "source": [
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwcQJDWIjomP",
        "outputId": "486e8ae0-59f9-4d96-a4f2-6409716c58f0"
      },
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/tcc/datasets/mypersonality/mypersonality_final.csv', encoding=\"ISO-8859-1\")\n",
        "df = df.drop(['#AUTHID',  'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'DATE', 'NETWORKSIZE', 'BETWEENNESS', 'NBETWEENNESS','DENSITY','BROKERAGE','NBROKERAGE','TRANSITIVITY'], axis = 1) \n",
        "df.cNEU.replace(to_replace=['n', 'y'], value=[0, 1], inplace=True)\n",
        "df.cEXT.replace(to_replace=['n', 'y'], value=[0, 1], inplace=True)\n",
        "df.cAGR.replace(to_replace=['n', 'y'], value=[0, 1], inplace=True)\n",
        "df.cCON.replace(to_replace=['n', 'y'], value=[0, 1], inplace=True)\n",
        "df.cOPN.replace(to_replace=['n', 'y'], value=[0, 1], inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c2Oo9MILTCu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f48960eb-19b8-4ebf-f989-4c707b6d2861"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STATUS</th>\n",
              "      <th>cEXT</th>\n",
              "      <th>cNEU</th>\n",
              "      <th>cAGR</th>\n",
              "      <th>cCON</th>\n",
              "      <th>cOPN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>likes the sound of thunder.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is so sleepy it's not even funny that's she can't get to sleep.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>is sore and wants the knot of muscles at the base of her neck to stop hurting. On the other hand, YAY I'M IN ILLINOIS! &lt;3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>likes how the day sounds in this new song.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is home. &lt;3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                      STATUS  ...  cOPN\n",
              "0  likes the sound of thunder.                                                                                                ...  1   \n",
              "1  is so sleepy it's not even funny that's she can't get to sleep.                                                            ...  1   \n",
              "2  is sore and wants the knot of muscles at the base of her neck to stop hurting. On the other hand, YAY I'M IN ILLINOIS! <3  ...  1   \n",
              "3  likes how the day sounds in this new song.                                                                                 ...  1   \n",
              "4  is home. <3                                                                                                                ...  1   \n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56VRHT9-Nabh",
        "outputId": "b98121c4-5164-47f0-efd6-b4ebff1bf981"
      },
      "source": [
        "#pre-processing\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "newStopWords = ['propname', 'im', 'propnames' '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "stop.extend(newStopWords)\n",
        "\n",
        "\n",
        "df['STATUS'] = df['STATUS'].str.lower() #lowercase\n",
        "df['STATUS'] = df['STATUS'].str.replace('[{}]'.format(string.punctuation), '') #ponctuaction\n",
        "df['STATUS'] = df['STATUS'].str.replace(r'\\d+','') #numbers\n",
        "df['STATUS'] = df['STATUS'].str.replace(' +', ' ') #this should replace all multiple spaces with a single space\n",
        "df['STATUS'] = df['STATUS'].str.strip() #remove all spaces from the start and end\n",
        "df['STATUS'] = df['STATUS'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop])) #stopwords\n",
        "\n",
        "# df['STATUS'] = [x.replace(\"propnames\", \"\") for x in df['STATUS']]\n",
        "# df['STATUS'] = [x.replace(\"propname\", \"\") for x in df['STATUS']]\n",
        "\n",
        "df.replace(\"\", np.nan, inplace=True) #empty lines to nan\n",
        "df.dropna(how='any', inplace=True) #remove nan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNiRWXpdM8Ai"
      },
      "source": [
        "#prepare EDA input to proccess in local computer (due to incompatibilities of the library with google collab)\n",
        "label_cols = [\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\"]\n",
        "df['one_hot_labels'] = df['cEXT'].map(str) + df['cNEU'].map(str) + df['cAGR'].map(str) + df['cCON'].map(str) + df['cOPN'].map(str)\n",
        "df = df.drop([\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\"], axis=1)\n",
        "frame = df[['one_hot_labels', 'STATUS']]\n",
        "frame.to_csv('input_mypersonalityEDA.txt', header=False, index=False, sep='\\t', mode='a') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYzHQxpoOBvA"
      },
      "source": [
        "#receives input processed by EDA (output)\n",
        "import re\n",
        "new_df = pd.read_csv('/content/output_mypersonalityEDA.txt', names=['content'], sep=\"\\f\", header=None)\n",
        "rows_list = []\n",
        "for index, row in new_df.iterrows():\n",
        "  props = re.split(r'\\t+', row['content'])\n",
        "  cEXT = props[0][0]\n",
        "  cNEU = props[0][1]\n",
        "  cAGR = props[0][2]\n",
        "  cCON = props[0][3]\n",
        "  cOPN = props[0][4]\n",
        "  STATUS = props[1]\n",
        "  dataTemp = {}\n",
        "  dataTemp.update({'cEXT':cEXT, 'cNEU':cNEU, 'cAGR':cAGR, 'cCON':cCON, 'cOPN':cOPN, 'STATUS':STATUS})\n",
        "  rows_list.append(dataTemp)\n",
        "\n",
        "df = pd.DataFrame(rows_list, columns=[\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\", \"STATUS\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiVtb4XIn6cB"
      },
      "source": [
        "df.STATUS = df.STATUS.astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsqG6Upc5rO3"
      },
      "source": [
        "df.STATUS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLqNvAlkstek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2440480-b504-4d1a-b7db-4772c64211cb"
      },
      "source": [
        "df.info()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 49315 entries, 0 to 49314\n",
            "Data columns (total 6 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   cEXT    49315 non-null  object\n",
            " 1   cNEU    49315 non-null  object\n",
            " 2   cAGR    49315 non-null  object\n",
            " 3   cCON    49315 non-null  object\n",
            " 4   cOPN    49315 non-null  object\n",
            " 5   STATUS  49315 non-null  object\n",
            "dtypes: object(6)\n",
            "memory usage: 2.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ-gbCwNmA4k"
      },
      "source": [
        "#split training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "list_classes = [\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\"]\n",
        "train, test = train_test_split(df, random_state=42, test_size=0.15, shuffle=True)\n",
        "\n",
        "list_sentences_train = train.STATUS\n",
        "list_sentences_test = test.STATUS\n",
        "\n",
        "y_train = train[list_classes].values\n",
        "y_test = test[list_classes].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFlMnREAmNdB"
      },
      "source": [
        "#tokenize data\n",
        "max_features = 5000 \n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features, lower=True, filters='') #Only the most common num_words-1 will kept, by default, all punctuation is removed, turning the texts into space-separated sequences of words\n",
        "\n",
        "tokenizer.fit_on_texts(list(list_sentences_train))\n",
        "\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_EqK-NqmQfu"
      },
      "source": [
        "# pad data\n",
        "maxlen = 20\n",
        "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqb7Poc-ky0w"
      },
      "source": [
        "import numpy as np\r\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr2uAaaeky3W"
      },
      "source": [
        "#function to load and convert and proccess dataset to Glove, Word2vec or FastText requirements\r\n",
        "def loadEmbeddingMatrix(typeToLoad):\r\n",
        "        #load different embedding fil depending on which embedding matrix are going to experiment with\r\n",
        "        if(typeToLoad==\"glove\"):\r\n",
        "            EMBEDDING_FILE='/content/drive/MyDrive/tcc/glove.6B.200d.txt'\r\n",
        "            embed_size = 200\r\n",
        "        elif(typeToLoad==\"word2vec\"):\r\n",
        "            word2vecDict = word2vec.KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\", binary=True)\r\n",
        "            embed_size = 300\r\n",
        "        elif(typeToLoad==\"fasttext\"):\r\n",
        "            EMBEDDING_FILE='../input/fasttext/wiki.simple.vec'\r\n",
        "            embed_size = 300\r\n",
        "\r\n",
        "        if(typeToLoad==\"glove\" or typeToLoad==\"fasttext\" ):\r\n",
        "            embeddings_index = dict()\r\n",
        "            #Transfer the embedding weights into a dictionary by iterating through every line of the file.\r\n",
        "            f = open(EMBEDDING_FILE)\r\n",
        "            for line in f:\r\n",
        "                #split up line into an indexed array\r\n",
        "                values = line.split()\r\n",
        "                #first index is word\r\n",
        "                word = values[0]\r\n",
        "                #store the rest of the values in the array as a new array\r\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "                embeddings_index[word] = coefs #50 dimensions\r\n",
        "            f.close()\r\n",
        "            print('Loaded %s word vectors.' % len(embeddings_index))\r\n",
        "        else:\r\n",
        "            embeddings_index = dict()\r\n",
        "            for word in word2vecDict.wv.vocab:\r\n",
        "                embeddings_index[word] = word2vecDict.word_vec(word)\r\n",
        "            print('Loaded %s word vectors.' % len(embeddings_index))\r\n",
        "            \r\n",
        "        gc.collect()\r\n",
        "        #We get the mean and standard deviation of the embedding weights so that we could maintain the \r\n",
        "        #same statistics for the rest of our own random generated weights. \r\n",
        "        all_embs = np.stack(list(embeddings_index.values()))\r\n",
        "        emb_mean,emb_std = all_embs.mean(), all_embs.std()\r\n",
        "        \r\n",
        "        nb_words = len(tokenizer.word_index)\r\n",
        "        #We are going to set the embedding size to the pretrained dimension as we are replicating it.\r\n",
        "        #the size will be Number of Words in Vocab X Embedding Size\r\n",
        "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\r\n",
        "        gc.collect()\r\n",
        "\r\n",
        "        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \r\n",
        "        #our own dictionary and loaded pretrained embedding. \r\n",
        "        embeddedCount = 0\r\n",
        "        for word, i in tokenizer.word_index.items():\r\n",
        "            i-=1\r\n",
        "            #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\r\n",
        "            embedding_vector = embeddings_index.get(word)\r\n",
        "            #and store inside the embedding matrix that we will train later on.\r\n",
        "            if embedding_vector is not None: \r\n",
        "                embedding_matrix[i] = embedding_vector\r\n",
        "                embeddedCount+=1\r\n",
        "        print('total embedded:',embeddedCount,'common words')\r\n",
        "        \r\n",
        "        del(embeddings_index)\r\n",
        "        gc.collect()\r\n",
        "        \r\n",
        "        #finally, return the embedding matrix\r\n",
        "        return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te_lKzLtky6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25743b7-331e-49f3-ecb6-c8036e10f79d"
      },
      "source": [
        "embedding_matrix = loadEmbeddingMatrix('glove')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n",
            "total embedded: 15651 common words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-1WHt7fky8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8933b07-36e6-48e3-df71-8b90438d57ee"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19597, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p61swCR6mVty"
      },
      "source": [
        "#We begin our defining an input layer \n",
        "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlXR-ZOpne1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8abfb9c-6ed6-4e84-c796-1b694ce5a6f7"
      },
      "source": [
        "#embed layer with glove\n",
        "\n",
        "#default comented\n",
        "#embed_size = 128\n",
        "#x = Embedding(max_features, embed_size)(inp)\n",
        "\n",
        "x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhz3QiLkrNhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d649c55d-3372-4140-ecc9-51593e98390a"
      },
      "source": [
        "#base hidden and output layers\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "#default\n",
        "#x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n",
        "x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\n",
        "\n",
        "x = GlobalMaxPool1D()(x)\n",
        "\n",
        "\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "# - 1 ou +1 (4)\n",
        "x = Dense(5, activation=\"sigmoid\")(x)\n",
        "\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
        "\n",
        "\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.AUC(), tf.keras.metrics.Recall() ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "redGqN3MrP1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa23b95-b0a4-4f9f-908d-1ad650120b41"
      },
      "source": [
        "#training and validation parameters\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "#ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "batch_size = 32 #refers to the number of training examples used in an iteration.\n",
        "epochs = 500\n",
        "history = model.fit(\n",
        "    X_train,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.10\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 37725 samples, validate on 4192 samples\n",
            "Epoch 1/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.6618 - accuracy: 0.5985 - precision: 0.6248 - auc: 0.6251 - recall: 0.4922 - val_loss: 0.6562 - val_accuracy: 0.6052 - val_precision: 0.6334 - val_auc: 0.6308 - val_recall: 0.4935\n",
            "Epoch 2/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.6544 - accuracy: 0.6055 - precision: 0.6362 - auc: 0.6333 - recall: 0.4908 - val_loss: 0.6417 - val_accuracy: 0.6205 - val_precision: 0.6423 - val_auc: 0.6374 - val_recall: 0.4862\n",
            "Epoch 3/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.6395 - accuracy: 0.6194 - precision: 0.6491 - auc: 0.6423 - recall: 0.4809 - val_loss: 0.6256 - val_accuracy: 0.6309 - val_precision: 0.6560 - val_auc: 0.6474 - val_recall: 0.4775\n",
            "Epoch 4/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.6234 - accuracy: 0.6343 - precision: 0.6621 - auc: 0.6531 - recall: 0.4770 - val_loss: 0.6065 - val_accuracy: 0.6532 - val_precision: 0.6678 - val_auc: 0.6588 - val_recall: 0.4781\n",
            "Epoch 5/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.6083 - accuracy: 0.6478 - precision: 0.6727 - auc: 0.6646 - recall: 0.4810 - val_loss: 0.5956 - val_accuracy: 0.6637 - val_precision: 0.6773 - val_auc: 0.6701 - val_recall: 0.4846\n",
            "Epoch 6/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.5966 - accuracy: 0.6583 - precision: 0.6810 - auc: 0.6754 - recall: 0.4892 - val_loss: 0.5839 - val_accuracy: 0.6722 - val_precision: 0.6843 - val_auc: 0.6805 - val_recall: 0.4941\n",
            "Epoch 7/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.5851 - accuracy: 0.6686 - precision: 0.6872 - auc: 0.6855 - recall: 0.4996 - val_loss: 0.5755 - val_accuracy: 0.6776 - val_precision: 0.6899 - val_auc: 0.6900 - val_recall: 0.5045\n",
            "Epoch 8/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.5745 - accuracy: 0.6751 - precision: 0.6924 - auc: 0.6945 - recall: 0.5096 - val_loss: 0.5640 - val_accuracy: 0.6844 - val_precision: 0.6944 - val_auc: 0.6986 - val_recall: 0.5144\n",
            "Epoch 9/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.5663 - accuracy: 0.6830 - precision: 0.6964 - auc: 0.7027 - recall: 0.5196 - val_loss: 0.5573 - val_accuracy: 0.6885 - val_precision: 0.6983 - val_auc: 0.7064 - val_recall: 0.5242\n",
            "Epoch 10/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.5579 - accuracy: 0.6877 - precision: 0.7003 - auc: 0.7101 - recall: 0.5284 - val_loss: 0.5503 - val_accuracy: 0.6960 - val_precision: 0.7020 - val_auc: 0.7135 - val_recall: 0.5325\n",
            "Epoch 11/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.5511 - accuracy: 0.6939 - precision: 0.7036 - auc: 0.7169 - recall: 0.5370 - val_loss: 0.5390 - val_accuracy: 0.7034 - val_precision: 0.7050 - val_auc: 0.7200 - val_recall: 0.5412\n",
            "Epoch 12/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.5447 - accuracy: 0.7008 - precision: 0.7064 - auc: 0.7232 - recall: 0.5457 - val_loss: 0.5342 - val_accuracy: 0.7101 - val_precision: 0.7076 - val_auc: 0.7261 - val_recall: 0.5501\n",
            "Epoch 13/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.5366 - accuracy: 0.7048 - precision: 0.7086 - auc: 0.7292 - recall: 0.5548 - val_loss: 0.5256 - val_accuracy: 0.7166 - val_precision: 0.7095 - val_auc: 0.7319 - val_recall: 0.5590\n",
            "Epoch 14/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.5315 - accuracy: 0.7094 - precision: 0.7105 - auc: 0.7347 - recall: 0.5632 - val_loss: 0.5255 - val_accuracy: 0.7226 - val_precision: 0.7114 - val_auc: 0.7373 - val_recall: 0.5673\n",
            "Epoch 15/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.5241 - accuracy: 0.7167 - precision: 0.7124 - auc: 0.7399 - recall: 0.5716 - val_loss: 0.5167 - val_accuracy: 0.7261 - val_precision: 0.7131 - val_auc: 0.7424 - val_recall: 0.5758\n",
            "Epoch 16/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.5181 - accuracy: 0.7207 - precision: 0.7140 - auc: 0.7449 - recall: 0.5799 - val_loss: 0.5099 - val_accuracy: 0.7305 - val_precision: 0.7148 - val_auc: 0.7472 - val_recall: 0.5837\n",
            "Epoch 17/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.5146 - accuracy: 0.7248 - precision: 0.7158 - auc: 0.7495 - recall: 0.5873 - val_loss: 0.5021 - val_accuracy: 0.7394 - val_precision: 0.7166 - val_auc: 0.7517 - val_recall: 0.5908\n",
            "Epoch 18/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.5090 - accuracy: 0.7292 - precision: 0.7175 - auc: 0.7539 - recall: 0.5945 - val_loss: 0.4974 - val_accuracy: 0.7431 - val_precision: 0.7183 - val_auc: 0.7560 - val_recall: 0.5978\n",
            "Epoch 19/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.5034 - accuracy: 0.7328 - precision: 0.7192 - auc: 0.7581 - recall: 0.6012 - val_loss: 0.4970 - val_accuracy: 0.7447 - val_precision: 0.7200 - val_auc: 0.7600 - val_recall: 0.6043\n",
            "Epoch 20/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4981 - accuracy: 0.7374 - precision: 0.7208 - auc: 0.7620 - recall: 0.6074 - val_loss: 0.4899 - val_accuracy: 0.7472 - val_precision: 0.7216 - val_auc: 0.7639 - val_recall: 0.6105\n",
            "Epoch 21/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4950 - accuracy: 0.7389 - precision: 0.7223 - auc: 0.7658 - recall: 0.6136 - val_loss: 0.4845 - val_accuracy: 0.7548 - val_precision: 0.7230 - val_auc: 0.7676 - val_recall: 0.6164\n",
            "Epoch 22/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4901 - accuracy: 0.7426 - precision: 0.7238 - auc: 0.7694 - recall: 0.6193 - val_loss: 0.4814 - val_accuracy: 0.7578 - val_precision: 0.7245 - val_auc: 0.7710 - val_recall: 0.6220\n",
            "Epoch 23/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4868 - accuracy: 0.7449 - precision: 0.7253 - auc: 0.7727 - recall: 0.6245 - val_loss: 0.4721 - val_accuracy: 0.7566 - val_precision: 0.7260 - val_auc: 0.7743 - val_recall: 0.6271\n",
            "Epoch 24/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4832 - accuracy: 0.7472 - precision: 0.7267 - auc: 0.7760 - recall: 0.6296 - val_loss: 0.4775 - val_accuracy: 0.7575 - val_precision: 0.7274 - val_auc: 0.7775 - val_recall: 0.6319\n",
            "Epoch 25/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4800 - accuracy: 0.7495 - precision: 0.7281 - auc: 0.7790 - recall: 0.6343 - val_loss: 0.4706 - val_accuracy: 0.7606 - val_precision: 0.7287 - val_auc: 0.7804 - val_recall: 0.6366\n",
            "Epoch 26/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4787 - accuracy: 0.7513 - precision: 0.7294 - auc: 0.7819 - recall: 0.6388 - val_loss: 0.4701 - val_accuracy: 0.7615 - val_precision: 0.7300 - val_auc: 0.7832 - val_recall: 0.6410\n",
            "Epoch 27/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4751 - accuracy: 0.7536 - precision: 0.7307 - auc: 0.7846 - recall: 0.6430 - val_loss: 0.4712 - val_accuracy: 0.7608 - val_precision: 0.7314 - val_auc: 0.7859 - val_recall: 0.6449\n",
            "Epoch 28/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4732 - accuracy: 0.7543 - precision: 0.7320 - auc: 0.7871 - recall: 0.6468 - val_loss: 0.4589 - val_accuracy: 0.7686 - val_precision: 0.7326 - val_auc: 0.7884 - val_recall: 0.6488\n",
            "Epoch 29/500\n",
            "37725/37725 [==============================] - 38s 1ms/step - loss: 0.4702 - accuracy: 0.7565 - precision: 0.7332 - auc: 0.7896 - recall: 0.6507 - val_loss: 0.4588 - val_accuracy: 0.7693 - val_precision: 0.7337 - val_auc: 0.7908 - val_recall: 0.6525\n",
            "Epoch 30/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4670 - accuracy: 0.7601 - precision: 0.7344 - auc: 0.7920 - recall: 0.6543 - val_loss: 0.4585 - val_accuracy: 0.7700 - val_precision: 0.7350 - val_auc: 0.7932 - val_recall: 0.6560\n",
            "Epoch 31/500\n",
            "37725/37725 [==============================] - 38s 1ms/step - loss: 0.4651 - accuracy: 0.7616 - precision: 0.7356 - auc: 0.7943 - recall: 0.6577 - val_loss: 0.4530 - val_accuracy: 0.7678 - val_precision: 0.7362 - val_auc: 0.7954 - val_recall: 0.6593\n",
            "Epoch 32/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4644 - accuracy: 0.7623 - precision: 0.7367 - auc: 0.7965 - recall: 0.6610 - val_loss: 0.4561 - val_accuracy: 0.7721 - val_precision: 0.7373 - val_auc: 0.7975 - val_recall: 0.6626\n",
            "Epoch 33/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4609 - accuracy: 0.7637 - precision: 0.7379 - auc: 0.7986 - recall: 0.6641 - val_loss: 0.4587 - val_accuracy: 0.7700 - val_precision: 0.7385 - val_auc: 0.7995 - val_recall: 0.6655\n",
            "Epoch 34/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4577 - accuracy: 0.7655 - precision: 0.7390 - auc: 0.8006 - recall: 0.6670 - val_loss: 0.4515 - val_accuracy: 0.7729 - val_precision: 0.7395 - val_auc: 0.8015 - val_recall: 0.6684\n",
            "Epoch 35/500\n",
            "37725/37725 [==============================] - 38s 1ms/step - loss: 0.4568 - accuracy: 0.7661 - precision: 0.7401 - auc: 0.8025 - recall: 0.6698 - val_loss: 0.4485 - val_accuracy: 0.7750 - val_precision: 0.7406 - val_auc: 0.8034 - val_recall: 0.6712\n",
            "Epoch 36/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4553 - accuracy: 0.7671 - precision: 0.7411 - auc: 0.8043 - recall: 0.6725 - val_loss: 0.4500 - val_accuracy: 0.7740 - val_precision: 0.7416 - val_auc: 0.8052 - val_recall: 0.6738\n",
            "Epoch 37/500\n",
            "37725/37725 [==============================] - 38s 1ms/step - loss: 0.4536 - accuracy: 0.7679 - precision: 0.7420 - auc: 0.8061 - recall: 0.6751 - val_loss: 0.4498 - val_accuracy: 0.7738 - val_precision: 0.7425 - val_auc: 0.8069 - val_recall: 0.6763\n",
            "Epoch 38/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4513 - accuracy: 0.7690 - precision: 0.7430 - auc: 0.8078 - recall: 0.6776 - val_loss: 0.4417 - val_accuracy: 0.7759 - val_precision: 0.7434 - val_auc: 0.8086 - val_recall: 0.6788\n",
            "Epoch 39/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4493 - accuracy: 0.7704 - precision: 0.7438 - auc: 0.8095 - recall: 0.6801 - val_loss: 0.4425 - val_accuracy: 0.7782 - val_precision: 0.7443 - val_auc: 0.8103 - val_recall: 0.6813\n",
            "Epoch 40/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4487 - accuracy: 0.7713 - precision: 0.7447 - auc: 0.8111 - recall: 0.6824 - val_loss: 0.4377 - val_accuracy: 0.7794 - val_precision: 0.7451 - val_auc: 0.8118 - val_recall: 0.6836\n",
            "Epoch 41/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4451 - accuracy: 0.7735 - precision: 0.7456 - auc: 0.8126 - recall: 0.6847 - val_loss: 0.4375 - val_accuracy: 0.7793 - val_precision: 0.7460 - val_auc: 0.8134 - val_recall: 0.6858\n",
            "Epoch 42/500\n",
            "37725/37725 [==============================] - 38s 1ms/step - loss: 0.4461 - accuracy: 0.7731 - precision: 0.7464 - auc: 0.8141 - recall: 0.6869 - val_loss: 0.4333 - val_accuracy: 0.7824 - val_precision: 0.7469 - val_auc: 0.8148 - val_recall: 0.6879\n",
            "Epoch 43/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4434 - accuracy: 0.7754 - precision: 0.7473 - auc: 0.8155 - recall: 0.6889 - val_loss: 0.4330 - val_accuracy: 0.7837 - val_precision: 0.7477 - val_auc: 0.8163 - val_recall: 0.6900\n",
            "Epoch 44/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4437 - accuracy: 0.7744 - precision: 0.7481 - auc: 0.8169 - recall: 0.6909 - val_loss: 0.4300 - val_accuracy: 0.7854 - val_precision: 0.7485 - val_auc: 0.8176 - val_recall: 0.6919\n",
            "Epoch 45/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4389 - accuracy: 0.7784 - precision: 0.7489 - auc: 0.8183 - recall: 0.6929 - val_loss: 0.4287 - val_accuracy: 0.7854 - val_precision: 0.7493 - val_auc: 0.8190 - val_recall: 0.6938\n",
            "Epoch 46/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4396 - accuracy: 0.7782 - precision: 0.7498 - auc: 0.8196 - recall: 0.6947 - val_loss: 0.4324 - val_accuracy: 0.7838 - val_precision: 0.7501 - val_auc: 0.8203 - val_recall: 0.6956\n",
            "Epoch 47/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4374 - accuracy: 0.7791 - precision: 0.7505 - auc: 0.8209 - recall: 0.6965 - val_loss: 0.4294 - val_accuracy: 0.7855 - val_precision: 0.7509 - val_auc: 0.8215 - val_recall: 0.6975\n",
            "Epoch 48/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4381 - accuracy: 0.7795 - precision: 0.7513 - auc: 0.8222 - recall: 0.6984 - val_loss: 0.4298 - val_accuracy: 0.7874 - val_precision: 0.7516 - val_auc: 0.8228 - val_recall: 0.6992\n",
            "Epoch 49/500\n",
            "37725/37725 [==============================] - 38s 1ms/step - loss: 0.4372 - accuracy: 0.7793 - precision: 0.7520 - auc: 0.8233 - recall: 0.7000 - val_loss: 0.4290 - val_accuracy: 0.7893 - val_precision: 0.7524 - val_auc: 0.8239 - val_recall: 0.7008\n",
            "Epoch 50/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4352 - accuracy: 0.7807 - precision: 0.7527 - auc: 0.8245 - recall: 0.7016 - val_loss: 0.4256 - val_accuracy: 0.7918 - val_precision: 0.7531 - val_auc: 0.8251 - val_recall: 0.7024\n",
            "Epoch 51/500\n",
            "37725/37725 [==============================] - 38s 1ms/step - loss: 0.4333 - accuracy: 0.7809 - precision: 0.7535 - auc: 0.8256 - recall: 0.7032 - val_loss: 0.4285 - val_accuracy: 0.7875 - val_precision: 0.7538 - val_auc: 0.8262 - val_recall: 0.7040\n",
            "Epoch 52/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4331 - accuracy: 0.7830 - precision: 0.7542 - auc: 0.8267 - recall: 0.7048 - val_loss: 0.4194 - val_accuracy: 0.7916 - val_precision: 0.7545 - val_auc: 0.8273 - val_recall: 0.7055\n",
            "Epoch 53/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4316 - accuracy: 0.7822 - precision: 0.7548 - auc: 0.8278 - recall: 0.7063 - val_loss: 0.4178 - val_accuracy: 0.7931 - val_precision: 0.7551 - val_auc: 0.8283 - val_recall: 0.7071\n",
            "Epoch 54/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4277 - accuracy: 0.7855 - precision: 0.7555 - auc: 0.8289 - recall: 0.7079 - val_loss: 0.4184 - val_accuracy: 0.7932 - val_precision: 0.7558 - val_auc: 0.8294 - val_recall: 0.7086\n",
            "Epoch 55/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4302 - accuracy: 0.7845 - precision: 0.7561 - auc: 0.8299 - recall: 0.7093 - val_loss: 0.4185 - val_accuracy: 0.7929 - val_precision: 0.7564 - val_auc: 0.8304 - val_recall: 0.7100\n",
            "Epoch 56/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4296 - accuracy: 0.7845 - precision: 0.7567 - auc: 0.8309 - recall: 0.7107 - val_loss: 0.4173 - val_accuracy: 0.7929 - val_precision: 0.7570 - val_auc: 0.8314 - val_recall: 0.7114\n",
            "Epoch 57/500\n",
            "37725/37725 [==============================] - 38s 1ms/step - loss: 0.4282 - accuracy: 0.7851 - precision: 0.7573 - auc: 0.8318 - recall: 0.7120 - val_loss: 0.4152 - val_accuracy: 0.7934 - val_precision: 0.7577 - val_auc: 0.8323 - val_recall: 0.7127\n",
            "Epoch 58/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4257 - accuracy: 0.7869 - precision: 0.7580 - auc: 0.8328 - recall: 0.7134 - val_loss: 0.4193 - val_accuracy: 0.7893 - val_precision: 0.7582 - val_auc: 0.8332 - val_recall: 0.7140\n",
            "Epoch 59/500\n",
            "37725/37725 [==============================] - 38s 1ms/step - loss: 0.4256 - accuracy: 0.7870 - precision: 0.7586 - auc: 0.8337 - recall: 0.7147 - val_loss: 0.4167 - val_accuracy: 0.7945 - val_precision: 0.7589 - val_auc: 0.8341 - val_recall: 0.7153\n",
            "Epoch 60/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4226 - accuracy: 0.7879 - precision: 0.7592 - auc: 0.8346 - recall: 0.7159 - val_loss: 0.4117 - val_accuracy: 0.7950 - val_precision: 0.7595 - val_auc: 0.8350 - val_recall: 0.7164\n",
            "Epoch 61/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4231 - accuracy: 0.7875 - precision: 0.7598 - auc: 0.8355 - recall: 0.7170 - val_loss: 0.4112 - val_accuracy: 0.7943 - val_precision: 0.7601 - val_auc: 0.8359 - val_recall: 0.7176\n",
            "Epoch 62/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4225 - accuracy: 0.7885 - precision: 0.7604 - auc: 0.8364 - recall: 0.7181 - val_loss: 0.4141 - val_accuracy: 0.7940 - val_precision: 0.7607 - val_auc: 0.8368 - val_recall: 0.7187\n",
            "Epoch 63/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4201 - accuracy: 0.7903 - precision: 0.7610 - auc: 0.8372 - recall: 0.7192 - val_loss: 0.4121 - val_accuracy: 0.7988 - val_precision: 0.7613 - val_auc: 0.8376 - val_recall: 0.7198\n",
            "Epoch 64/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4206 - accuracy: 0.7900 - precision: 0.7616 - auc: 0.8380 - recall: 0.7203 - val_loss: 0.4070 - val_accuracy: 0.7965 - val_precision: 0.7619 - val_auc: 0.8384 - val_recall: 0.7208\n",
            "Epoch 65/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4194 - accuracy: 0.7905 - precision: 0.7622 - auc: 0.8388 - recall: 0.7214 - val_loss: 0.4112 - val_accuracy: 0.7979 - val_precision: 0.7624 - val_auc: 0.8392 - val_recall: 0.7219\n",
            "Epoch 66/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4173 - accuracy: 0.7928 - precision: 0.7627 - auc: 0.8396 - recall: 0.7224 - val_loss: 0.4088 - val_accuracy: 0.7998 - val_precision: 0.7630 - val_auc: 0.8400 - val_recall: 0.7230\n",
            "Epoch 67/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4167 - accuracy: 0.7924 - precision: 0.7633 - auc: 0.8404 - recall: 0.7235 - val_loss: 0.4101 - val_accuracy: 0.7981 - val_precision: 0.7635 - val_auc: 0.8408 - val_recall: 0.7240\n",
            "Epoch 68/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4181 - accuracy: 0.7926 - precision: 0.7638 - auc: 0.8412 - recall: 0.7245 - val_loss: 0.4042 - val_accuracy: 0.8026 - val_precision: 0.7641 - val_auc: 0.8415 - val_recall: 0.7249\n",
            "Epoch 69/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.4167 - accuracy: 0.7932 - precision: 0.7644 - auc: 0.8419 - recall: 0.7254 - val_loss: 0.4101 - val_accuracy: 0.7982 - val_precision: 0.7647 - val_auc: 0.8423 - val_recall: 0.7258\n",
            "Epoch 70/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4158 - accuracy: 0.7926 - precision: 0.7650 - auc: 0.8426 - recall: 0.7262 - val_loss: 0.4026 - val_accuracy: 0.8007 - val_precision: 0.7653 - val_auc: 0.8430 - val_recall: 0.7266\n",
            "Epoch 71/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.4133 - accuracy: 0.7936 - precision: 0.7656 - auc: 0.8433 - recall: 0.7270 - val_loss: 0.4041 - val_accuracy: 0.7992 - val_precision: 0.7658 - val_auc: 0.8437 - val_recall: 0.7275\n",
            "Epoch 72/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4136 - accuracy: 0.7955 - precision: 0.7661 - auc: 0.8440 - recall: 0.7279 - val_loss: 0.4051 - val_accuracy: 0.7982 - val_precision: 0.7664 - val_auc: 0.8444 - val_recall: 0.7283\n",
            "Epoch 73/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4150 - accuracy: 0.7932 - precision: 0.7667 - auc: 0.8447 - recall: 0.7287 - val_loss: 0.4056 - val_accuracy: 0.7982 - val_precision: 0.7669 - val_auc: 0.8450 - val_recall: 0.7291\n",
            "Epoch 74/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4126 - accuracy: 0.7947 - precision: 0.7672 - auc: 0.8454 - recall: 0.7295 - val_loss: 0.4057 - val_accuracy: 0.7989 - val_precision: 0.7674 - val_auc: 0.8457 - val_recall: 0.7299\n",
            "Epoch 75/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4124 - accuracy: 0.7945 - precision: 0.7677 - auc: 0.8460 - recall: 0.7303 - val_loss: 0.4033 - val_accuracy: 0.8000 - val_precision: 0.7679 - val_auc: 0.8463 - val_recall: 0.7307\n",
            "Epoch 76/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4111 - accuracy: 0.7961 - precision: 0.7682 - auc: 0.8467 - recall: 0.7311 - val_loss: 0.4041 - val_accuracy: 0.8018 - val_precision: 0.7684 - val_auc: 0.8470 - val_recall: 0.7314\n",
            "Epoch 77/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4105 - accuracy: 0.7970 - precision: 0.7687 - auc: 0.8473 - recall: 0.7318 - val_loss: 0.4032 - val_accuracy: 0.8001 - val_precision: 0.7689 - val_auc: 0.8476 - val_recall: 0.7322\n",
            "Epoch 78/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4098 - accuracy: 0.7976 - precision: 0.7692 - auc: 0.8479 - recall: 0.7325 - val_loss: 0.4025 - val_accuracy: 0.8013 - val_precision: 0.7694 - val_auc: 0.8482 - val_recall: 0.7329\n",
            "Epoch 79/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4102 - accuracy: 0.7966 - precision: 0.7697 - auc: 0.8485 - recall: 0.7333 - val_loss: 0.4005 - val_accuracy: 0.8049 - val_precision: 0.7699 - val_auc: 0.8488 - val_recall: 0.7336\n",
            "Epoch 80/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4086 - accuracy: 0.7982 - precision: 0.7702 - auc: 0.8491 - recall: 0.7340 - val_loss: 0.4000 - val_accuracy: 0.8043 - val_precision: 0.7704 - val_auc: 0.8494 - val_recall: 0.7343\n",
            "Epoch 81/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4085 - accuracy: 0.7989 - precision: 0.7707 - auc: 0.8497 - recall: 0.7347 - val_loss: 0.3964 - val_accuracy: 0.8058 - val_precision: 0.7709 - val_auc: 0.8500 - val_recall: 0.7350\n",
            "Epoch 82/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4066 - accuracy: 0.7998 - precision: 0.7711 - auc: 0.8503 - recall: 0.7354 - val_loss: 0.3983 - val_accuracy: 0.8066 - val_precision: 0.7714 - val_auc: 0.8506 - val_recall: 0.7357\n",
            "Epoch 83/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4055 - accuracy: 0.7994 - precision: 0.7716 - auc: 0.8509 - recall: 0.7361 - val_loss: 0.3991 - val_accuracy: 0.8052 - val_precision: 0.7718 - val_auc: 0.8511 - val_recall: 0.7364\n",
            "Epoch 84/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.4066 - accuracy: 0.7991 - precision: 0.7721 - auc: 0.8514 - recall: 0.7368 - val_loss: 0.4008 - val_accuracy: 0.8053 - val_precision: 0.7723 - val_auc: 0.8517 - val_recall: 0.7371\n",
            "Epoch 85/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4050 - accuracy: 0.8009 - precision: 0.7725 - auc: 0.8520 - recall: 0.7374 - val_loss: 0.3950 - val_accuracy: 0.8085 - val_precision: 0.7727 - val_auc: 0.8522 - val_recall: 0.7378\n",
            "Epoch 86/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4044 - accuracy: 0.8010 - precision: 0.7730 - auc: 0.8525 - recall: 0.7381 - val_loss: 0.3985 - val_accuracy: 0.8042 - val_precision: 0.7732 - val_auc: 0.8528 - val_recall: 0.7384\n",
            "Epoch 87/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4048 - accuracy: 0.7999 - precision: 0.7734 - auc: 0.8530 - recall: 0.7387 - val_loss: 0.3952 - val_accuracy: 0.8063 - val_precision: 0.7736 - val_auc: 0.8533 - val_recall: 0.7390\n",
            "Epoch 88/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4028 - accuracy: 0.8011 - precision: 0.7738 - auc: 0.8535 - recall: 0.7393 - val_loss: 0.3946 - val_accuracy: 0.8076 - val_precision: 0.7741 - val_auc: 0.8538 - val_recall: 0.7396\n",
            "Epoch 89/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4030 - accuracy: 0.8017 - precision: 0.7743 - auc: 0.8541 - recall: 0.7400 - val_loss: 0.3974 - val_accuracy: 0.8061 - val_precision: 0.7745 - val_auc: 0.8543 - val_recall: 0.7403\n",
            "Epoch 90/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4012 - accuracy: 0.8021 - precision: 0.7747 - auc: 0.8546 - recall: 0.7406 - val_loss: 0.3921 - val_accuracy: 0.8072 - val_precision: 0.7749 - val_auc: 0.8548 - val_recall: 0.7409\n",
            "Epoch 91/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4030 - accuracy: 0.8015 - precision: 0.7751 - auc: 0.8551 - recall: 0.7412 - val_loss: 0.3940 - val_accuracy: 0.8074 - val_precision: 0.7753 - val_auc: 0.8553 - val_recall: 0.7415\n",
            "Epoch 92/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4028 - accuracy: 0.8024 - precision: 0.7755 - auc: 0.8556 - recall: 0.7418 - val_loss: 0.3934 - val_accuracy: 0.8076 - val_precision: 0.7757 - val_auc: 0.8558 - val_recall: 0.7421\n",
            "Epoch 93/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4009 - accuracy: 0.8030 - precision: 0.7759 - auc: 0.8560 - recall: 0.7424 - val_loss: 0.3881 - val_accuracy: 0.8089 - val_precision: 0.7761 - val_auc: 0.8563 - val_recall: 0.7426\n",
            "Epoch 94/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4006 - accuracy: 0.8035 - precision: 0.7763 - auc: 0.8565 - recall: 0.7429 - val_loss: 0.3927 - val_accuracy: 0.8093 - val_precision: 0.7765 - val_auc: 0.8567 - val_recall: 0.7432\n",
            "Epoch 95/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4003 - accuracy: 0.8034 - precision: 0.7767 - auc: 0.8570 - recall: 0.7435 - val_loss: 0.3911 - val_accuracy: 0.8077 - val_precision: 0.7769 - val_auc: 0.8572 - val_recall: 0.7438\n",
            "Epoch 96/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4018 - accuracy: 0.8036 - precision: 0.7771 - auc: 0.8574 - recall: 0.7440 - val_loss: 0.3936 - val_accuracy: 0.8094 - val_precision: 0.7773 - val_auc: 0.8576 - val_recall: 0.7443\n",
            "Epoch 97/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3998 - accuracy: 0.8042 - precision: 0.7775 - auc: 0.8579 - recall: 0.7446 - val_loss: 0.3948 - val_accuracy: 0.8087 - val_precision: 0.7777 - val_auc: 0.8581 - val_recall: 0.7449\n",
            "Epoch 98/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.4004 - accuracy: 0.8029 - precision: 0.7779 - auc: 0.8583 - recall: 0.7451 - val_loss: 0.3894 - val_accuracy: 0.8113 - val_precision: 0.7781 - val_auc: 0.8585 - val_recall: 0.7454\n",
            "Epoch 99/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3992 - accuracy: 0.8042 - precision: 0.7782 - auc: 0.8587 - recall: 0.7456 - val_loss: 0.3891 - val_accuracy: 0.8092 - val_precision: 0.7784 - val_auc: 0.8590 - val_recall: 0.7459\n",
            "Epoch 100/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3971 - accuracy: 0.8051 - precision: 0.7786 - auc: 0.8592 - recall: 0.7462 - val_loss: 0.3899 - val_accuracy: 0.8095 - val_precision: 0.7788 - val_auc: 0.8594 - val_recall: 0.7464\n",
            "Epoch 101/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3982 - accuracy: 0.8046 - precision: 0.7790 - auc: 0.8596 - recall: 0.7467 - val_loss: 0.3911 - val_accuracy: 0.8085 - val_precision: 0.7791 - val_auc: 0.8598 - val_recall: 0.7469\n",
            "Epoch 102/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3995 - accuracy: 0.8029 - precision: 0.7793 - auc: 0.8600 - recall: 0.7472 - val_loss: 0.3910 - val_accuracy: 0.8094 - val_precision: 0.7795 - val_auc: 0.8602 - val_recall: 0.7474\n",
            "Epoch 103/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3956 - accuracy: 0.8060 - precision: 0.7797 - auc: 0.8604 - recall: 0.7476 - val_loss: 0.3888 - val_accuracy: 0.8103 - val_precision: 0.7798 - val_auc: 0.8606 - val_recall: 0.7479\n",
            "Epoch 104/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3968 - accuracy: 0.8055 - precision: 0.7800 - auc: 0.8608 - recall: 0.7481 - val_loss: 0.3856 - val_accuracy: 0.8152 - val_precision: 0.7802 - val_auc: 0.8610 - val_recall: 0.7484\n",
            "Epoch 105/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3941 - accuracy: 0.8062 - precision: 0.7804 - auc: 0.8612 - recall: 0.7486 - val_loss: 0.3899 - val_accuracy: 0.8087 - val_precision: 0.7805 - val_auc: 0.8614 - val_recall: 0.7488\n",
            "Epoch 106/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3949 - accuracy: 0.8060 - precision: 0.7807 - auc: 0.8616 - recall: 0.7491 - val_loss: 0.3857 - val_accuracy: 0.8115 - val_precision: 0.7809 - val_auc: 0.8618 - val_recall: 0.7493\n",
            "Epoch 107/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3941 - accuracy: 0.8075 - precision: 0.7811 - auc: 0.8620 - recall: 0.7495 - val_loss: 0.3833 - val_accuracy: 0.8136 - val_precision: 0.7812 - val_auc: 0.8622 - val_recall: 0.7498\n",
            "Epoch 108/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3935 - accuracy: 0.8075 - precision: 0.7814 - auc: 0.8624 - recall: 0.7500 - val_loss: 0.3875 - val_accuracy: 0.8087 - val_precision: 0.7816 - val_auc: 0.8626 - val_recall: 0.7502\n",
            "Epoch 109/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3929 - accuracy: 0.8071 - precision: 0.7817 - auc: 0.8628 - recall: 0.7505 - val_loss: 0.3839 - val_accuracy: 0.8108 - val_precision: 0.7819 - val_auc: 0.8630 - val_recall: 0.7507\n",
            "Epoch 110/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3943 - accuracy: 0.8071 - precision: 0.7821 - auc: 0.8632 - recall: 0.7509 - val_loss: 0.3864 - val_accuracy: 0.8127 - val_precision: 0.7822 - val_auc: 0.8634 - val_recall: 0.7511\n",
            "Epoch 111/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3934 - accuracy: 0.8077 - precision: 0.7824 - auc: 0.8635 - recall: 0.7514 - val_loss: 0.3864 - val_accuracy: 0.8124 - val_precision: 0.7825 - val_auc: 0.8637 - val_recall: 0.7516\n",
            "Epoch 112/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3927 - accuracy: 0.8082 - precision: 0.7827 - auc: 0.8639 - recall: 0.7518 - val_loss: 0.3845 - val_accuracy: 0.8139 - val_precision: 0.7829 - val_auc: 0.8641 - val_recall: 0.7520\n",
            "Epoch 113/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3930 - accuracy: 0.8076 - precision: 0.7830 - auc: 0.8643 - recall: 0.7522 - val_loss: 0.3854 - val_accuracy: 0.8145 - val_precision: 0.7832 - val_auc: 0.8644 - val_recall: 0.7524\n",
            "Epoch 114/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3922 - accuracy: 0.8091 - precision: 0.7833 - auc: 0.8646 - recall: 0.7527 - val_loss: 0.3831 - val_accuracy: 0.8149 - val_precision: 0.7835 - val_auc: 0.8648 - val_recall: 0.7529\n",
            "Epoch 115/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3932 - accuracy: 0.8070 - precision: 0.7836 - auc: 0.8650 - recall: 0.7531 - val_loss: 0.3855 - val_accuracy: 0.8114 - val_precision: 0.7838 - val_auc: 0.8651 - val_recall: 0.7533\n",
            "Epoch 116/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3907 - accuracy: 0.8092 - precision: 0.7839 - auc: 0.8653 - recall: 0.7535 - val_loss: 0.3843 - val_accuracy: 0.8125 - val_precision: 0.7841 - val_auc: 0.8655 - val_recall: 0.7537\n",
            "Epoch 117/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3911 - accuracy: 0.8093 - precision: 0.7842 - auc: 0.8657 - recall: 0.7539 - val_loss: 0.3801 - val_accuracy: 0.8166 - val_precision: 0.7844 - val_auc: 0.8658 - val_recall: 0.7541\n",
            "Epoch 118/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3909 - accuracy: 0.8092 - precision: 0.7845 - auc: 0.8660 - recall: 0.7543 - val_loss: 0.3823 - val_accuracy: 0.8155 - val_precision: 0.7847 - val_auc: 0.8662 - val_recall: 0.7545\n",
            "Epoch 119/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3920 - accuracy: 0.8087 - precision: 0.7848 - auc: 0.8663 - recall: 0.7547 - val_loss: 0.3802 - val_accuracy: 0.8154 - val_precision: 0.7850 - val_auc: 0.8665 - val_recall: 0.7549\n",
            "Epoch 120/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3914 - accuracy: 0.8103 - precision: 0.7851 - auc: 0.8667 - recall: 0.7551 - val_loss: 0.3829 - val_accuracy: 0.8157 - val_precision: 0.7853 - val_auc: 0.8668 - val_recall: 0.7553\n",
            "Epoch 121/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3895 - accuracy: 0.8099 - precision: 0.7855 - auc: 0.8670 - recall: 0.7555 - val_loss: 0.3821 - val_accuracy: 0.8151 - val_precision: 0.7856 - val_auc: 0.8671 - val_recall: 0.7556\n",
            "Epoch 122/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3894 - accuracy: 0.8114 - precision: 0.7857 - auc: 0.8673 - recall: 0.7558 - val_loss: 0.3832 - val_accuracy: 0.8160 - val_precision: 0.7859 - val_auc: 0.8675 - val_recall: 0.7560\n",
            "Epoch 123/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3909 - accuracy: 0.8095 - precision: 0.7860 - auc: 0.8676 - recall: 0.7562 - val_loss: 0.3816 - val_accuracy: 0.8156 - val_precision: 0.7862 - val_auc: 0.8678 - val_recall: 0.7564\n",
            "Epoch 124/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3906 - accuracy: 0.8099 - precision: 0.7863 - auc: 0.8679 - recall: 0.7566 - val_loss: 0.3841 - val_accuracy: 0.8174 - val_precision: 0.7865 - val_auc: 0.8681 - val_recall: 0.7568\n",
            "Epoch 125/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3882 - accuracy: 0.8111 - precision: 0.7866 - auc: 0.8682 - recall: 0.7570 - val_loss: 0.3806 - val_accuracy: 0.8159 - val_precision: 0.7867 - val_auc: 0.8684 - val_recall: 0.7571\n",
            "Epoch 126/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3875 - accuracy: 0.8122 - precision: 0.7869 - auc: 0.8685 - recall: 0.7573 - val_loss: 0.3846 - val_accuracy: 0.8141 - val_precision: 0.7870 - val_auc: 0.8687 - val_recall: 0.7575\n",
            "Epoch 127/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3874 - accuracy: 0.8115 - precision: 0.7872 - auc: 0.8688 - recall: 0.7577 - val_loss: 0.3767 - val_accuracy: 0.8164 - val_precision: 0.7873 - val_auc: 0.8690 - val_recall: 0.7579\n",
            "Epoch 128/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3893 - accuracy: 0.8113 - precision: 0.7874 - auc: 0.8692 - recall: 0.7581 - val_loss: 0.3823 - val_accuracy: 0.8166 - val_precision: 0.7876 - val_auc: 0.8693 - val_recall: 0.7582\n",
            "Epoch 129/500\n",
            "37725/37725 [==============================] - 39s 1ms/step - loss: 0.3882 - accuracy: 0.8119 - precision: 0.7877 - auc: 0.8694 - recall: 0.7584 - val_loss: 0.3799 - val_accuracy: 0.8171 - val_precision: 0.7878 - val_auc: 0.8696 - val_recall: 0.7586\n",
            "Epoch 130/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3873 - accuracy: 0.8115 - precision: 0.7880 - auc: 0.8697 - recall: 0.7587 - val_loss: 0.3782 - val_accuracy: 0.8167 - val_precision: 0.7881 - val_auc: 0.8699 - val_recall: 0.7589\n",
            "Epoch 131/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3864 - accuracy: 0.8121 - precision: 0.7882 - auc: 0.8700 - recall: 0.7591 - val_loss: 0.3779 - val_accuracy: 0.8183 - val_precision: 0.7884 - val_auc: 0.8702 - val_recall: 0.7592\n",
            "Epoch 132/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3878 - accuracy: 0.8104 - precision: 0.7885 - auc: 0.8703 - recall: 0.7594 - val_loss: 0.3773 - val_accuracy: 0.8167 - val_precision: 0.7886 - val_auc: 0.8704 - val_recall: 0.7596\n",
            "Epoch 133/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3859 - accuracy: 0.8122 - precision: 0.7888 - auc: 0.8706 - recall: 0.7598 - val_loss: 0.3783 - val_accuracy: 0.8151 - val_precision: 0.7889 - val_auc: 0.8707 - val_recall: 0.7599\n",
            "Epoch 134/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3872 - accuracy: 0.8120 - precision: 0.7890 - auc: 0.8709 - recall: 0.7601 - val_loss: 0.3768 - val_accuracy: 0.8189 - val_precision: 0.7891 - val_auc: 0.8710 - val_recall: 0.7602\n",
            "Epoch 135/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3862 - accuracy: 0.8129 - precision: 0.7893 - auc: 0.8711 - recall: 0.7604 - val_loss: 0.3814 - val_accuracy: 0.8156 - val_precision: 0.7894 - val_auc: 0.8713 - val_recall: 0.7605\n",
            "Epoch 136/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3865 - accuracy: 0.8132 - precision: 0.7895 - auc: 0.8714 - recall: 0.7607 - val_loss: 0.3753 - val_accuracy: 0.8156 - val_precision: 0.7896 - val_auc: 0.8715 - val_recall: 0.7609\n",
            "Epoch 137/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3847 - accuracy: 0.8126 - precision: 0.7898 - auc: 0.8717 - recall: 0.7610 - val_loss: 0.3800 - val_accuracy: 0.8165 - val_precision: 0.7899 - val_auc: 0.8718 - val_recall: 0.7612\n",
            "Epoch 138/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3858 - accuracy: 0.8144 - precision: 0.7900 - auc: 0.8719 - recall: 0.7613 - val_loss: 0.3761 - val_accuracy: 0.8186 - val_precision: 0.7902 - val_auc: 0.8721 - val_recall: 0.7615\n",
            "Epoch 139/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3848 - accuracy: 0.8140 - precision: 0.7903 - auc: 0.8722 - recall: 0.7616 - val_loss: 0.3805 - val_accuracy: 0.8161 - val_precision: 0.7904 - val_auc: 0.8723 - val_recall: 0.7618\n",
            "Epoch 140/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3837 - accuracy: 0.8135 - precision: 0.7905 - auc: 0.8725 - recall: 0.7620 - val_loss: 0.3780 - val_accuracy: 0.8202 - val_precision: 0.7906 - val_auc: 0.8726 - val_recall: 0.7621\n",
            "Epoch 141/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3864 - accuracy: 0.8121 - precision: 0.7908 - auc: 0.8727 - recall: 0.7623 - val_loss: 0.3792 - val_accuracy: 0.8182 - val_precision: 0.7909 - val_auc: 0.8728 - val_recall: 0.7624\n",
            "Epoch 142/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3842 - accuracy: 0.8137 - precision: 0.7910 - auc: 0.8730 - recall: 0.7626 - val_loss: 0.3780 - val_accuracy: 0.8178 - val_precision: 0.7911 - val_auc: 0.8731 - val_recall: 0.7627\n",
            "Epoch 143/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3829 - accuracy: 0.8148 - precision: 0.7912 - auc: 0.8732 - recall: 0.7629 - val_loss: 0.3780 - val_accuracy: 0.8158 - val_precision: 0.7914 - val_auc: 0.8733 - val_recall: 0.7630\n",
            "Epoch 144/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3853 - accuracy: 0.8129 - precision: 0.7915 - auc: 0.8735 - recall: 0.7632 - val_loss: 0.3772 - val_accuracy: 0.8188 - val_precision: 0.7916 - val_auc: 0.8736 - val_recall: 0.7633\n",
            "Epoch 145/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3828 - accuracy: 0.8144 - precision: 0.7917 - auc: 0.8737 - recall: 0.7635 - val_loss: 0.3754 - val_accuracy: 0.8192 - val_precision: 0.7918 - val_auc: 0.8738 - val_recall: 0.7636\n",
            "Epoch 146/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3811 - accuracy: 0.8151 - precision: 0.7919 - auc: 0.8740 - recall: 0.7638 - val_loss: 0.3757 - val_accuracy: 0.8177 - val_precision: 0.7920 - val_auc: 0.8741 - val_recall: 0.7639\n",
            "Epoch 147/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3838 - accuracy: 0.8151 - precision: 0.7921 - auc: 0.8742 - recall: 0.7641 - val_loss: 0.3767 - val_accuracy: 0.8193 - val_precision: 0.7923 - val_auc: 0.8743 - val_recall: 0.7642\n",
            "Epoch 148/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3832 - accuracy: 0.8147 - precision: 0.7924 - auc: 0.8744 - recall: 0.7644 - val_loss: 0.3763 - val_accuracy: 0.8180 - val_precision: 0.7925 - val_auc: 0.8746 - val_recall: 0.7645\n",
            "Epoch 149/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3832 - accuracy: 0.8147 - precision: 0.7926 - auc: 0.8747 - recall: 0.7646 - val_loss: 0.3734 - val_accuracy: 0.8205 - val_precision: 0.7927 - val_auc: 0.8748 - val_recall: 0.7648\n",
            "Epoch 150/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3831 - accuracy: 0.8153 - precision: 0.7928 - auc: 0.8749 - recall: 0.7649 - val_loss: 0.3702 - val_accuracy: 0.8211 - val_precision: 0.7929 - val_auc: 0.8750 - val_recall: 0.7651\n",
            "Epoch 151/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3822 - accuracy: 0.8157 - precision: 0.7930 - auc: 0.8751 - recall: 0.7652 - val_loss: 0.3759 - val_accuracy: 0.8187 - val_precision: 0.7931 - val_auc: 0.8753 - val_recall: 0.7653\n",
            "Epoch 152/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3808 - accuracy: 0.8153 - precision: 0.7932 - auc: 0.8754 - recall: 0.7655 - val_loss: 0.3751 - val_accuracy: 0.8202 - val_precision: 0.7934 - val_auc: 0.8755 - val_recall: 0.7656\n",
            "Epoch 153/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3847 - accuracy: 0.8138 - precision: 0.7935 - auc: 0.8756 - recall: 0.7657 - val_loss: 0.3741 - val_accuracy: 0.8198 - val_precision: 0.7936 - val_auc: 0.8757 - val_recall: 0.7659\n",
            "Epoch 154/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3821 - accuracy: 0.8153 - precision: 0.7937 - auc: 0.8758 - recall: 0.7660 - val_loss: 0.3740 - val_accuracy: 0.8219 - val_precision: 0.7938 - val_auc: 0.8759 - val_recall: 0.7661\n",
            "Epoch 155/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3814 - accuracy: 0.8160 - precision: 0.7939 - auc: 0.8760 - recall: 0.7663 - val_loss: 0.3743 - val_accuracy: 0.8226 - val_precision: 0.7940 - val_auc: 0.8761 - val_recall: 0.7664\n",
            "Epoch 156/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3821 - accuracy: 0.8151 - precision: 0.7941 - auc: 0.8762 - recall: 0.7665 - val_loss: 0.3743 - val_accuracy: 0.8180 - val_precision: 0.7942 - val_auc: 0.8764 - val_recall: 0.7667\n",
            "Epoch 157/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3837 - accuracy: 0.8145 - precision: 0.7943 - auc: 0.8765 - recall: 0.7668 - val_loss: 0.3727 - val_accuracy: 0.8198 - val_precision: 0.7944 - val_auc: 0.8766 - val_recall: 0.7669\n",
            "Epoch 158/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3793 - accuracy: 0.8163 - precision: 0.7945 - auc: 0.8767 - recall: 0.7670 - val_loss: 0.3735 - val_accuracy: 0.8194 - val_precision: 0.7946 - val_auc: 0.8768 - val_recall: 0.7671\n",
            "Epoch 159/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3794 - accuracy: 0.8167 - precision: 0.7947 - auc: 0.8769 - recall: 0.7672 - val_loss: 0.3734 - val_accuracy: 0.8216 - val_precision: 0.7948 - val_auc: 0.8770 - val_recall: 0.7674\n",
            "Epoch 160/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3794 - accuracy: 0.8175 - precision: 0.7949 - auc: 0.8771 - recall: 0.7675 - val_loss: 0.3727 - val_accuracy: 0.8206 - val_precision: 0.7950 - val_auc: 0.8772 - val_recall: 0.7676\n",
            "Epoch 161/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3799 - accuracy: 0.8177 - precision: 0.7951 - auc: 0.8773 - recall: 0.7677 - val_loss: 0.3718 - val_accuracy: 0.8202 - val_precision: 0.7953 - val_auc: 0.8774 - val_recall: 0.7679\n",
            "Epoch 162/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3813 - accuracy: 0.8159 - precision: 0.7954 - auc: 0.8775 - recall: 0.7680 - val_loss: 0.3756 - val_accuracy: 0.8186 - val_precision: 0.7954 - val_auc: 0.8776 - val_recall: 0.7681\n",
            "Epoch 163/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3798 - accuracy: 0.8158 - precision: 0.7955 - auc: 0.8777 - recall: 0.7682 - val_loss: 0.3719 - val_accuracy: 0.8217 - val_precision: 0.7956 - val_auc: 0.8778 - val_recall: 0.7683\n",
            "Epoch 164/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3817 - accuracy: 0.8152 - precision: 0.7957 - auc: 0.8779 - recall: 0.7685 - val_loss: 0.3759 - val_accuracy: 0.8186 - val_precision: 0.7958 - val_auc: 0.8780 - val_recall: 0.7686\n",
            "Epoch 165/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3802 - accuracy: 0.8156 - precision: 0.7959 - auc: 0.8781 - recall: 0.7687 - val_loss: 0.3769 - val_accuracy: 0.8176 - val_precision: 0.7960 - val_auc: 0.8782 - val_recall: 0.7688\n",
            "Epoch 166/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3781 - accuracy: 0.8185 - precision: 0.7961 - auc: 0.8783 - recall: 0.7689 - val_loss: 0.3762 - val_accuracy: 0.8187 - val_precision: 0.7962 - val_auc: 0.8784 - val_recall: 0.7690\n",
            "Epoch 167/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3804 - accuracy: 0.8179 - precision: 0.7963 - auc: 0.8785 - recall: 0.7691 - val_loss: 0.3752 - val_accuracy: 0.8184 - val_precision: 0.7964 - val_auc: 0.8786 - val_recall: 0.7693\n",
            "Epoch 168/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3793 - accuracy: 0.8168 - precision: 0.7965 - auc: 0.8787 - recall: 0.7694 - val_loss: 0.3737 - val_accuracy: 0.8212 - val_precision: 0.7966 - val_auc: 0.8788 - val_recall: 0.7695\n",
            "Epoch 169/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3799 - accuracy: 0.8176 - precision: 0.7967 - auc: 0.8789 - recall: 0.7696 - val_loss: 0.3709 - val_accuracy: 0.8199 - val_precision: 0.7968 - val_auc: 0.8790 - val_recall: 0.7697\n",
            "Epoch 170/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3767 - accuracy: 0.8197 - precision: 0.7969 - auc: 0.8791 - recall: 0.7698 - val_loss: 0.3726 - val_accuracy: 0.8212 - val_precision: 0.7970 - val_auc: 0.8792 - val_recall: 0.7699\n",
            "Epoch 171/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3787 - accuracy: 0.8182 - precision: 0.7971 - auc: 0.8793 - recall: 0.7700 - val_loss: 0.3683 - val_accuracy: 0.8236 - val_precision: 0.7972 - val_auc: 0.8794 - val_recall: 0.7701\n",
            "Epoch 172/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3759 - accuracy: 0.8194 - precision: 0.7973 - auc: 0.8795 - recall: 0.7703 - val_loss: 0.3720 - val_accuracy: 0.8222 - val_precision: 0.7974 - val_auc: 0.8796 - val_recall: 0.7704\n",
            "Epoch 173/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3783 - accuracy: 0.8188 - precision: 0.7975 - auc: 0.8796 - recall: 0.7705 - val_loss: 0.3674 - val_accuracy: 0.8247 - val_precision: 0.7976 - val_auc: 0.8797 - val_recall: 0.7706\n",
            "Epoch 174/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3785 - accuracy: 0.8183 - precision: 0.7977 - auc: 0.8798 - recall: 0.7707 - val_loss: 0.3686 - val_accuracy: 0.8246 - val_precision: 0.7977 - val_auc: 0.8799 - val_recall: 0.7708\n",
            "Epoch 175/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3775 - accuracy: 0.8189 - precision: 0.7978 - auc: 0.8800 - recall: 0.7710 - val_loss: 0.3698 - val_accuracy: 0.8234 - val_precision: 0.7979 - val_auc: 0.8801 - val_recall: 0.7711\n",
            "Epoch 176/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3775 - accuracy: 0.8174 - precision: 0.7980 - auc: 0.8802 - recall: 0.7712 - val_loss: 0.3737 - val_accuracy: 0.8201 - val_precision: 0.7981 - val_auc: 0.8803 - val_recall: 0.7713\n",
            "Epoch 177/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3778 - accuracy: 0.8188 - precision: 0.7982 - auc: 0.8804 - recall: 0.7714 - val_loss: 0.3687 - val_accuracy: 0.8220 - val_precision: 0.7983 - val_auc: 0.8805 - val_recall: 0.7715\n",
            "Epoch 178/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3764 - accuracy: 0.8198 - precision: 0.7984 - auc: 0.8806 - recall: 0.7716 - val_loss: 0.3692 - val_accuracy: 0.8226 - val_precision: 0.7985 - val_auc: 0.8806 - val_recall: 0.7717\n",
            "Epoch 179/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3768 - accuracy: 0.8188 - precision: 0.7985 - auc: 0.8807 - recall: 0.7718 - val_loss: 0.3670 - val_accuracy: 0.8221 - val_precision: 0.7986 - val_auc: 0.8808 - val_recall: 0.7719\n",
            "Epoch 180/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3758 - accuracy: 0.8189 - precision: 0.7987 - auc: 0.8809 - recall: 0.7720 - val_loss: 0.3659 - val_accuracy: 0.8259 - val_precision: 0.7988 - val_auc: 0.8810 - val_recall: 0.7721\n",
            "Epoch 181/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3762 - accuracy: 0.8211 - precision: 0.7989 - auc: 0.8811 - recall: 0.7722 - val_loss: 0.3675 - val_accuracy: 0.8254 - val_precision: 0.7990 - val_auc: 0.8812 - val_recall: 0.7724\n",
            "Epoch 182/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3771 - accuracy: 0.8190 - precision: 0.7991 - auc: 0.8813 - recall: 0.7725 - val_loss: 0.3719 - val_accuracy: 0.8216 - val_precision: 0.7992 - val_auc: 0.8813 - val_recall: 0.7726\n",
            "Epoch 183/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3757 - accuracy: 0.8202 - precision: 0.7992 - auc: 0.8814 - recall: 0.7727 - val_loss: 0.3669 - val_accuracy: 0.8271 - val_precision: 0.7993 - val_auc: 0.8815 - val_recall: 0.7728\n",
            "Epoch 184/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3751 - accuracy: 0.8201 - precision: 0.7994 - auc: 0.8816 - recall: 0.7729 - val_loss: 0.3610 - val_accuracy: 0.8273 - val_precision: 0.7995 - val_auc: 0.8817 - val_recall: 0.7730\n",
            "Epoch 185/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3771 - accuracy: 0.8195 - precision: 0.7996 - auc: 0.8818 - recall: 0.7731 - val_loss: 0.3663 - val_accuracy: 0.8273 - val_precision: 0.7997 - val_auc: 0.8819 - val_recall: 0.7732\n",
            "Epoch 186/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3758 - accuracy: 0.8210 - precision: 0.7998 - auc: 0.8819 - recall: 0.7733 - val_loss: 0.3665 - val_accuracy: 0.8263 - val_precision: 0.7999 - val_auc: 0.8820 - val_recall: 0.7734\n",
            "Epoch 187/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3757 - accuracy: 0.8209 - precision: 0.8000 - auc: 0.8821 - recall: 0.7735 - val_loss: 0.3688 - val_accuracy: 0.8250 - val_precision: 0.8000 - val_auc: 0.8822 - val_recall: 0.7736\n",
            "Epoch 188/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3753 - accuracy: 0.8203 - precision: 0.8001 - auc: 0.8823 - recall: 0.7737 - val_loss: 0.3685 - val_accuracy: 0.8271 - val_precision: 0.8002 - val_auc: 0.8824 - val_recall: 0.7738\n",
            "Epoch 189/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3739 - accuracy: 0.8210 - precision: 0.8003 - auc: 0.8824 - recall: 0.7739 - val_loss: 0.3644 - val_accuracy: 0.8270 - val_precision: 0.8004 - val_auc: 0.8825 - val_recall: 0.7740\n",
            "Epoch 190/500\n",
            "37725/37725 [==============================] - 40s 1ms/step - loss: 0.3739 - accuracy: 0.8221 - precision: 0.8005 - auc: 0.8826 - recall: 0.7741 - val_loss: 0.3652 - val_accuracy: 0.8260 - val_precision: 0.8005 - val_auc: 0.8827 - val_recall: 0.7742\n",
            "Epoch 191/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3739 - accuracy: 0.8208 - precision: 0.8006 - auc: 0.8828 - recall: 0.7743 - val_loss: 0.3654 - val_accuracy: 0.8246 - val_precision: 0.8007 - val_auc: 0.8829 - val_recall: 0.7744\n",
            "Epoch 192/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3739 - accuracy: 0.8212 - precision: 0.8008 - auc: 0.8829 - recall: 0.7745 - val_loss: 0.3659 - val_accuracy: 0.8221 - val_precision: 0.8009 - val_auc: 0.8830 - val_recall: 0.7746\n",
            "Epoch 193/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3750 - accuracy: 0.8190 - precision: 0.8009 - auc: 0.8831 - recall: 0.7747 - val_loss: 0.3628 - val_accuracy: 0.8266 - val_precision: 0.8010 - val_auc: 0.8832 - val_recall: 0.7748\n",
            "Epoch 194/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3735 - accuracy: 0.8213 - precision: 0.8011 - auc: 0.8833 - recall: 0.7749 - val_loss: 0.3652 - val_accuracy: 0.8262 - val_precision: 0.8012 - val_auc: 0.8833 - val_recall: 0.7750\n",
            "Epoch 195/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3735 - accuracy: 0.8211 - precision: 0.8013 - auc: 0.8834 - recall: 0.7751 - val_loss: 0.3621 - val_accuracy: 0.8262 - val_precision: 0.8013 - val_auc: 0.8835 - val_recall: 0.7752\n",
            "Epoch 196/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3740 - accuracy: 0.8212 - precision: 0.8014 - auc: 0.8836 - recall: 0.7752 - val_loss: 0.3606 - val_accuracy: 0.8283 - val_precision: 0.8015 - val_auc: 0.8836 - val_recall: 0.7753\n",
            "Epoch 197/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3736 - accuracy: 0.8214 - precision: 0.8016 - auc: 0.8837 - recall: 0.7754 - val_loss: 0.3650 - val_accuracy: 0.8254 - val_precision: 0.8017 - val_auc: 0.8838 - val_recall: 0.7755\n",
            "Epoch 198/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3721 - accuracy: 0.8220 - precision: 0.8017 - auc: 0.8839 - recall: 0.7756 - val_loss: 0.3624 - val_accuracy: 0.8272 - val_precision: 0.8018 - val_auc: 0.8840 - val_recall: 0.7757\n",
            "Epoch 199/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3718 - accuracy: 0.8224 - precision: 0.8019 - auc: 0.8840 - recall: 0.7758 - val_loss: 0.3617 - val_accuracy: 0.8266 - val_precision: 0.8020 - val_auc: 0.8841 - val_recall: 0.7759\n",
            "Epoch 200/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3732 - accuracy: 0.8208 - precision: 0.8020 - auc: 0.8842 - recall: 0.7760 - val_loss: 0.3635 - val_accuracy: 0.8243 - val_precision: 0.8021 - val_auc: 0.8843 - val_recall: 0.7761\n",
            "Epoch 201/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3740 - accuracy: 0.8212 - precision: 0.8022 - auc: 0.8843 - recall: 0.7762 - val_loss: 0.3631 - val_accuracy: 0.8254 - val_precision: 0.8023 - val_auc: 0.8844 - val_recall: 0.7763\n",
            "Epoch 202/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3729 - accuracy: 0.8218 - precision: 0.8023 - auc: 0.8845 - recall: 0.7764 - val_loss: 0.3624 - val_accuracy: 0.8294 - val_precision: 0.8024 - val_auc: 0.8846 - val_recall: 0.7764\n",
            "Epoch 203/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3718 - accuracy: 0.8215 - precision: 0.8025 - auc: 0.8846 - recall: 0.7765 - val_loss: 0.3631 - val_accuracy: 0.8267 - val_precision: 0.8026 - val_auc: 0.8847 - val_recall: 0.7766\n",
            "Epoch 204/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3710 - accuracy: 0.8221 - precision: 0.8026 - auc: 0.8848 - recall: 0.7767 - val_loss: 0.3642 - val_accuracy: 0.8259 - val_precision: 0.8027 - val_auc: 0.8849 - val_recall: 0.7768\n",
            "Epoch 205/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3748 - accuracy: 0.8205 - precision: 0.8028 - auc: 0.8849 - recall: 0.7769 - val_loss: 0.3645 - val_accuracy: 0.8259 - val_precision: 0.8029 - val_auc: 0.8850 - val_recall: 0.7770\n",
            "Epoch 206/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3719 - accuracy: 0.8223 - precision: 0.8029 - auc: 0.8851 - recall: 0.7771 - val_loss: 0.3606 - val_accuracy: 0.8271 - val_precision: 0.8030 - val_auc: 0.8851 - val_recall: 0.7772\n",
            "Epoch 207/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3722 - accuracy: 0.8220 - precision: 0.8031 - auc: 0.8852 - recall: 0.7773 - val_loss: 0.3641 - val_accuracy: 0.8258 - val_precision: 0.8031 - val_auc: 0.8853 - val_recall: 0.7773\n",
            "Epoch 208/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3709 - accuracy: 0.8228 - precision: 0.8032 - auc: 0.8854 - recall: 0.7774 - val_loss: 0.3598 - val_accuracy: 0.8281 - val_precision: 0.8033 - val_auc: 0.8854 - val_recall: 0.7775\n",
            "Epoch 209/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3707 - accuracy: 0.8237 - precision: 0.8034 - auc: 0.8855 - recall: 0.7776 - val_loss: 0.3621 - val_accuracy: 0.8275 - val_precision: 0.8035 - val_auc: 0.8856 - val_recall: 0.7777\n",
            "Epoch 210/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3695 - accuracy: 0.8227 - precision: 0.8035 - auc: 0.8856 - recall: 0.7777 - val_loss: 0.3640 - val_accuracy: 0.8246 - val_precision: 0.8036 - val_auc: 0.8857 - val_recall: 0.7778\n",
            "Epoch 211/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3733 - accuracy: 0.8225 - precision: 0.8037 - auc: 0.8858 - recall: 0.7779 - val_loss: 0.3605 - val_accuracy: 0.8276 - val_precision: 0.8037 - val_auc: 0.8858 - val_recall: 0.7780\n",
            "Epoch 212/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3718 - accuracy: 0.8229 - precision: 0.8038 - auc: 0.8859 - recall: 0.7781 - val_loss: 0.3642 - val_accuracy: 0.8252 - val_precision: 0.8039 - val_auc: 0.8860 - val_recall: 0.7782\n",
            "Epoch 213/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3727 - accuracy: 0.8229 - precision: 0.8040 - auc: 0.8861 - recall: 0.7782 - val_loss: 0.3638 - val_accuracy: 0.8277 - val_precision: 0.8040 - val_auc: 0.8861 - val_recall: 0.7783\n",
            "Epoch 214/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3730 - accuracy: 0.8227 - precision: 0.8041 - auc: 0.8862 - recall: 0.7784 - val_loss: 0.3623 - val_accuracy: 0.8271 - val_precision: 0.8042 - val_auc: 0.8863 - val_recall: 0.7785\n",
            "Epoch 215/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3706 - accuracy: 0.8230 - precision: 0.8042 - auc: 0.8863 - recall: 0.7786 - val_loss: 0.3618 - val_accuracy: 0.8276 - val_precision: 0.8043 - val_auc: 0.8864 - val_recall: 0.7787\n",
            "Epoch 216/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3689 - accuracy: 0.8234 - precision: 0.8044 - auc: 0.8865 - recall: 0.7787 - val_loss: 0.3639 - val_accuracy: 0.8289 - val_precision: 0.8044 - val_auc: 0.8865 - val_recall: 0.7788\n",
            "Epoch 217/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3706 - accuracy: 0.8241 - precision: 0.8045 - auc: 0.8866 - recall: 0.7789 - val_loss: 0.3606 - val_accuracy: 0.8274 - val_precision: 0.8046 - val_auc: 0.8867 - val_recall: 0.7790\n",
            "Epoch 218/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3733 - accuracy: 0.8224 - precision: 0.8047 - auc: 0.8867 - recall: 0.7790 - val_loss: 0.3607 - val_accuracy: 0.8275 - val_precision: 0.8047 - val_auc: 0.8868 - val_recall: 0.7791\n",
            "Epoch 219/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3702 - accuracy: 0.8235 - precision: 0.8048 - auc: 0.8869 - recall: 0.7792 - val_loss: 0.3591 - val_accuracy: 0.8273 - val_precision: 0.8048 - val_auc: 0.8869 - val_recall: 0.7793\n",
            "Epoch 220/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3713 - accuracy: 0.8237 - precision: 0.8049 - auc: 0.8870 - recall: 0.7794 - val_loss: 0.3591 - val_accuracy: 0.8303 - val_precision: 0.8050 - val_auc: 0.8870 - val_recall: 0.7795\n",
            "Epoch 221/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3690 - accuracy: 0.8238 - precision: 0.8051 - auc: 0.8871 - recall: 0.7795 - val_loss: 0.3600 - val_accuracy: 0.8299 - val_precision: 0.8051 - val_auc: 0.8872 - val_recall: 0.7796\n",
            "Epoch 222/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3689 - accuracy: 0.8257 - precision: 0.8052 - auc: 0.8872 - recall: 0.7797 - val_loss: 0.3584 - val_accuracy: 0.8289 - val_precision: 0.8053 - val_auc: 0.8873 - val_recall: 0.7798\n",
            "Epoch 223/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3697 - accuracy: 0.8237 - precision: 0.8054 - auc: 0.8874 - recall: 0.7798 - val_loss: 0.3604 - val_accuracy: 0.8307 - val_precision: 0.8054 - val_auc: 0.8874 - val_recall: 0.7799\n",
            "Epoch 224/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3690 - accuracy: 0.8242 - precision: 0.8055 - auc: 0.8875 - recall: 0.7800 - val_loss: 0.3613 - val_accuracy: 0.8287 - val_precision: 0.8056 - val_auc: 0.8876 - val_recall: 0.7800\n",
            "Epoch 225/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3697 - accuracy: 0.8245 - precision: 0.8056 - auc: 0.8876 - recall: 0.7801 - val_loss: 0.3610 - val_accuracy: 0.8279 - val_precision: 0.8057 - val_auc: 0.8877 - val_recall: 0.7802\n",
            "Epoch 226/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3712 - accuracy: 0.8232 - precision: 0.8058 - auc: 0.8878 - recall: 0.7803 - val_loss: 0.3621 - val_accuracy: 0.8275 - val_precision: 0.8058 - val_auc: 0.8878 - val_recall: 0.7803\n",
            "Epoch 227/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3698 - accuracy: 0.8235 - precision: 0.8059 - auc: 0.8879 - recall: 0.7804 - val_loss: 0.3592 - val_accuracy: 0.8282 - val_precision: 0.8060 - val_auc: 0.8879 - val_recall: 0.7805\n",
            "Epoch 228/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3691 - accuracy: 0.8249 - precision: 0.8060 - auc: 0.8880 - recall: 0.7805 - val_loss: 0.3616 - val_accuracy: 0.8294 - val_precision: 0.8061 - val_auc: 0.8881 - val_recall: 0.7806\n",
            "Epoch 229/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3709 - accuracy: 0.8221 - precision: 0.8062 - auc: 0.8881 - recall: 0.7807 - val_loss: 0.3598 - val_accuracy: 0.8295 - val_precision: 0.8062 - val_auc: 0.8882 - val_recall: 0.7807\n",
            "Epoch 230/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3689 - accuracy: 0.8249 - precision: 0.8063 - auc: 0.8882 - recall: 0.7808 - val_loss: 0.3619 - val_accuracy: 0.8267 - val_precision: 0.8064 - val_auc: 0.8883 - val_recall: 0.7809\n",
            "Epoch 231/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3705 - accuracy: 0.8246 - precision: 0.8064 - auc: 0.8884 - recall: 0.7810 - val_loss: 0.3598 - val_accuracy: 0.8293 - val_precision: 0.8065 - val_auc: 0.8884 - val_recall: 0.7810\n",
            "Epoch 232/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3686 - accuracy: 0.8247 - precision: 0.8065 - auc: 0.8885 - recall: 0.7811 - val_loss: 0.3593 - val_accuracy: 0.8297 - val_precision: 0.8066 - val_auc: 0.8885 - val_recall: 0.7812\n",
            "Epoch 233/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3692 - accuracy: 0.8246 - precision: 0.8067 - auc: 0.8886 - recall: 0.7813 - val_loss: 0.3576 - val_accuracy: 0.8284 - val_precision: 0.8067 - val_auc: 0.8887 - val_recall: 0.7813\n",
            "Epoch 234/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3712 - accuracy: 0.8241 - precision: 0.8068 - auc: 0.8887 - recall: 0.7814 - val_loss: 0.3583 - val_accuracy: 0.8293 - val_precision: 0.8069 - val_auc: 0.8888 - val_recall: 0.7815\n",
            "Epoch 235/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3687 - accuracy: 0.8246 - precision: 0.8069 - auc: 0.8888 - recall: 0.7815 - val_loss: 0.3584 - val_accuracy: 0.8303 - val_precision: 0.8070 - val_auc: 0.8889 - val_recall: 0.7816\n",
            "Epoch 236/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3713 - accuracy: 0.8238 - precision: 0.8071 - auc: 0.8890 - recall: 0.7817 - val_loss: 0.3613 - val_accuracy: 0.8278 - val_precision: 0.8071 - val_auc: 0.8890 - val_recall: 0.7817\n",
            "Epoch 237/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3683 - accuracy: 0.8246 - precision: 0.8072 - auc: 0.8891 - recall: 0.7818 - val_loss: 0.3573 - val_accuracy: 0.8286 - val_precision: 0.8073 - val_auc: 0.8891 - val_recall: 0.7819\n",
            "Epoch 238/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3676 - accuracy: 0.8258 - precision: 0.8073 - auc: 0.8892 - recall: 0.7819 - val_loss: 0.3586 - val_accuracy: 0.8281 - val_precision: 0.8074 - val_auc: 0.8892 - val_recall: 0.7820\n",
            "Epoch 239/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3686 - accuracy: 0.8247 - precision: 0.8074 - auc: 0.8893 - recall: 0.7821 - val_loss: 0.3606 - val_accuracy: 0.8290 - val_precision: 0.8075 - val_auc: 0.8894 - val_recall: 0.7821\n",
            "Epoch 240/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3672 - accuracy: 0.8267 - precision: 0.8076 - auc: 0.8894 - recall: 0.7822 - val_loss: 0.3605 - val_accuracy: 0.8287 - val_precision: 0.8076 - val_auc: 0.8895 - val_recall: 0.7823\n",
            "Epoch 241/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3686 - accuracy: 0.8250 - precision: 0.8077 - auc: 0.8895 - recall: 0.7823 - val_loss: 0.3611 - val_accuracy: 0.8288 - val_precision: 0.8077 - val_auc: 0.8896 - val_recall: 0.7824\n",
            "Epoch 242/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3678 - accuracy: 0.8257 - precision: 0.8078 - auc: 0.8896 - recall: 0.7825 - val_loss: 0.3606 - val_accuracy: 0.8258 - val_precision: 0.8079 - val_auc: 0.8897 - val_recall: 0.7825\n",
            "Epoch 243/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3684 - accuracy: 0.8247 - precision: 0.8079 - auc: 0.8897 - recall: 0.7826 - val_loss: 0.3614 - val_accuracy: 0.8270 - val_precision: 0.8080 - val_auc: 0.8898 - val_recall: 0.7826\n",
            "Epoch 244/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3683 - accuracy: 0.8255 - precision: 0.8081 - auc: 0.8899 - recall: 0.7827 - val_loss: 0.3588 - val_accuracy: 0.8275 - val_precision: 0.8081 - val_auc: 0.8899 - val_recall: 0.7828\n",
            "Epoch 245/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3677 - accuracy: 0.8258 - precision: 0.8082 - auc: 0.8900 - recall: 0.7828 - val_loss: 0.3601 - val_accuracy: 0.8275 - val_precision: 0.8082 - val_auc: 0.8900 - val_recall: 0.7829\n",
            "Epoch 246/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3698 - accuracy: 0.8247 - precision: 0.8083 - auc: 0.8901 - recall: 0.7830 - val_loss: 0.3588 - val_accuracy: 0.8290 - val_precision: 0.8084 - val_auc: 0.8901 - val_recall: 0.7830\n",
            "Epoch 247/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3663 - accuracy: 0.8263 - precision: 0.8084 - auc: 0.8902 - recall: 0.7831 - val_loss: 0.3585 - val_accuracy: 0.8286 - val_precision: 0.8085 - val_auc: 0.8902 - val_recall: 0.7832\n",
            "Epoch 248/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3657 - accuracy: 0.8270 - precision: 0.8085 - auc: 0.8903 - recall: 0.7832 - val_loss: 0.3571 - val_accuracy: 0.8284 - val_precision: 0.8086 - val_auc: 0.8903 - val_recall: 0.7833\n",
            "Epoch 249/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3659 - accuracy: 0.8255 - precision: 0.8087 - auc: 0.8904 - recall: 0.7834 - val_loss: 0.3581 - val_accuracy: 0.8282 - val_precision: 0.8087 - val_auc: 0.8905 - val_recall: 0.7834\n",
            "Epoch 250/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3648 - accuracy: 0.8268 - precision: 0.8088 - auc: 0.8905 - recall: 0.7835 - val_loss: 0.3573 - val_accuracy: 0.8294 - val_precision: 0.8088 - val_auc: 0.8906 - val_recall: 0.7836\n",
            "Epoch 251/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3667 - accuracy: 0.8258 - precision: 0.8089 - auc: 0.8906 - recall: 0.7836 - val_loss: 0.3602 - val_accuracy: 0.8252 - val_precision: 0.8089 - val_auc: 0.8907 - val_recall: 0.7837\n",
            "Epoch 252/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3652 - accuracy: 0.8273 - precision: 0.8090 - auc: 0.8907 - recall: 0.7837 - val_loss: 0.3567 - val_accuracy: 0.8266 - val_precision: 0.8090 - val_auc: 0.8908 - val_recall: 0.7838\n",
            "Epoch 253/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3656 - accuracy: 0.8263 - precision: 0.8091 - auc: 0.8908 - recall: 0.7839 - val_loss: 0.3571 - val_accuracy: 0.8285 - val_precision: 0.8092 - val_auc: 0.8909 - val_recall: 0.7839\n",
            "Epoch 254/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3673 - accuracy: 0.8263 - precision: 0.8092 - auc: 0.8909 - recall: 0.7840 - val_loss: 0.3588 - val_accuracy: 0.8310 - val_precision: 0.8093 - val_auc: 0.8910 - val_recall: 0.7841\n",
            "Epoch 255/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3659 - accuracy: 0.8272 - precision: 0.8093 - auc: 0.8910 - recall: 0.7841 - val_loss: 0.3592 - val_accuracy: 0.8313 - val_precision: 0.8094 - val_auc: 0.8911 - val_recall: 0.7842\n",
            "Epoch 256/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3672 - accuracy: 0.8258 - precision: 0.8095 - auc: 0.8912 - recall: 0.7842 - val_loss: 0.3570 - val_accuracy: 0.8299 - val_precision: 0.8095 - val_auc: 0.8912 - val_recall: 0.7843\n",
            "Epoch 257/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3650 - accuracy: 0.8276 - precision: 0.8096 - auc: 0.8913 - recall: 0.7844 - val_loss: 0.3560 - val_accuracy: 0.8310 - val_precision: 0.8096 - val_auc: 0.8913 - val_recall: 0.7844\n",
            "Epoch 258/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3643 - accuracy: 0.8275 - precision: 0.8097 - auc: 0.8914 - recall: 0.7845 - val_loss: 0.3589 - val_accuracy: 0.8297 - val_precision: 0.8097 - val_auc: 0.8914 - val_recall: 0.7845\n",
            "Epoch 259/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3653 - accuracy: 0.8273 - precision: 0.8098 - auc: 0.8915 - recall: 0.7846 - val_loss: 0.3556 - val_accuracy: 0.8312 - val_precision: 0.8099 - val_auc: 0.8915 - val_recall: 0.7847\n",
            "Epoch 260/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3650 - accuracy: 0.8272 - precision: 0.8099 - auc: 0.8916 - recall: 0.7847 - val_loss: 0.3553 - val_accuracy: 0.8289 - val_precision: 0.8100 - val_auc: 0.8916 - val_recall: 0.7848\n",
            "Epoch 261/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3639 - accuracy: 0.8283 - precision: 0.8100 - auc: 0.8917 - recall: 0.7848 - val_loss: 0.3535 - val_accuracy: 0.8298 - val_precision: 0.8101 - val_auc: 0.8917 - val_recall: 0.7849\n",
            "Epoch 262/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3651 - accuracy: 0.8270 - precision: 0.8102 - auc: 0.8918 - recall: 0.7850 - val_loss: 0.3521 - val_accuracy: 0.8309 - val_precision: 0.8102 - val_auc: 0.8918 - val_recall: 0.7850\n",
            "Epoch 263/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3652 - accuracy: 0.8276 - precision: 0.8103 - auc: 0.8919 - recall: 0.7851 - val_loss: 0.3530 - val_accuracy: 0.8312 - val_precision: 0.8103 - val_auc: 0.8919 - val_recall: 0.7851\n",
            "Epoch 264/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3654 - accuracy: 0.8275 - precision: 0.8104 - auc: 0.8920 - recall: 0.7852 - val_loss: 0.3537 - val_accuracy: 0.8302 - val_precision: 0.8104 - val_auc: 0.8920 - val_recall: 0.7852\n",
            "Epoch 265/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3656 - accuracy: 0.8279 - precision: 0.8105 - auc: 0.8921 - recall: 0.7853 - val_loss: 0.3526 - val_accuracy: 0.8319 - val_precision: 0.8106 - val_auc: 0.8921 - val_recall: 0.7853\n",
            "Epoch 266/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3648 - accuracy: 0.8285 - precision: 0.8106 - auc: 0.8922 - recall: 0.7854 - val_loss: 0.3537 - val_accuracy: 0.8315 - val_precision: 0.8107 - val_auc: 0.8922 - val_recall: 0.7854\n",
            "Epoch 267/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3636 - accuracy: 0.8287 - precision: 0.8107 - auc: 0.8923 - recall: 0.7855 - val_loss: 0.3521 - val_accuracy: 0.8342 - val_precision: 0.8108 - val_auc: 0.8923 - val_recall: 0.7856\n",
            "Epoch 268/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3621 - accuracy: 0.8293 - precision: 0.8109 - auc: 0.8924 - recall: 0.7856 - val_loss: 0.3514 - val_accuracy: 0.8346 - val_precision: 0.8109 - val_auc: 0.8924 - val_recall: 0.7857\n",
            "Epoch 269/500\n",
            "37725/37725 [==============================] - 41s 1ms/step - loss: 0.3616 - accuracy: 0.8292 - precision: 0.8110 - auc: 0.8925 - recall: 0.7857 - val_loss: 0.3529 - val_accuracy: 0.8311 - val_precision: 0.8110 - val_auc: 0.8925 - val_recall: 0.7858\n",
            "Epoch 270/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3645 - accuracy: 0.8281 - precision: 0.8111 - auc: 0.8926 - recall: 0.7858 - val_loss: 0.3499 - val_accuracy: 0.8346 - val_precision: 0.8112 - val_auc: 0.8926 - val_recall: 0.7859\n",
            "Epoch 271/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3641 - accuracy: 0.8290 - precision: 0.8112 - auc: 0.8927 - recall: 0.7860 - val_loss: 0.3513 - val_accuracy: 0.8334 - val_precision: 0.8113 - val_auc: 0.8927 - val_recall: 0.7860\n",
            "Epoch 272/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3629 - accuracy: 0.8294 - precision: 0.8113 - auc: 0.8928 - recall: 0.7861 - val_loss: 0.3511 - val_accuracy: 0.8325 - val_precision: 0.8114 - val_auc: 0.8928 - val_recall: 0.7861\n",
            "Epoch 273/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3651 - accuracy: 0.8273 - precision: 0.8114 - auc: 0.8929 - recall: 0.7862 - val_loss: 0.3512 - val_accuracy: 0.8319 - val_precision: 0.8115 - val_auc: 0.8929 - val_recall: 0.7862\n",
            "Epoch 274/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3642 - accuracy: 0.8278 - precision: 0.8115 - auc: 0.8930 - recall: 0.7863 - val_loss: 0.3506 - val_accuracy: 0.8336 - val_precision: 0.8116 - val_auc: 0.8930 - val_recall: 0.7863\n",
            "Epoch 275/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3640 - accuracy: 0.8291 - precision: 0.8116 - auc: 0.8931 - recall: 0.7864 - val_loss: 0.3479 - val_accuracy: 0.8344 - val_precision: 0.8117 - val_auc: 0.8931 - val_recall: 0.7865\n",
            "Epoch 276/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3650 - accuracy: 0.8279 - precision: 0.8117 - auc: 0.8932 - recall: 0.7865 - val_loss: 0.3495 - val_accuracy: 0.8340 - val_precision: 0.8118 - val_auc: 0.8932 - val_recall: 0.7866\n",
            "Epoch 277/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3646 - accuracy: 0.8283 - precision: 0.8119 - auc: 0.8932 - recall: 0.7866 - val_loss: 0.3501 - val_accuracy: 0.8368 - val_precision: 0.8119 - val_auc: 0.8933 - val_recall: 0.7867\n",
            "Epoch 278/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3647 - accuracy: 0.8266 - precision: 0.8120 - auc: 0.8933 - recall: 0.7867 - val_loss: 0.3494 - val_accuracy: 0.8345 - val_precision: 0.8120 - val_auc: 0.8934 - val_recall: 0.7868\n",
            "Epoch 279/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3623 - accuracy: 0.8293 - precision: 0.8121 - auc: 0.8934 - recall: 0.7868 - val_loss: 0.3479 - val_accuracy: 0.8330 - val_precision: 0.8121 - val_auc: 0.8935 - val_recall: 0.7869\n",
            "Epoch 280/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3637 - accuracy: 0.8283 - precision: 0.8122 - auc: 0.8935 - recall: 0.7869 - val_loss: 0.3496 - val_accuracy: 0.8328 - val_precision: 0.8122 - val_auc: 0.8936 - val_recall: 0.7870\n",
            "Epoch 281/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3606 - accuracy: 0.8298 - precision: 0.8123 - auc: 0.8936 - recall: 0.7870 - val_loss: 0.3514 - val_accuracy: 0.8344 - val_precision: 0.8123 - val_auc: 0.8937 - val_recall: 0.7871\n",
            "Epoch 282/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3626 - accuracy: 0.8299 - precision: 0.8124 - auc: 0.8937 - recall: 0.7871 - val_loss: 0.3535 - val_accuracy: 0.8345 - val_precision: 0.8124 - val_auc: 0.8938 - val_recall: 0.7872\n",
            "Epoch 283/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3634 - accuracy: 0.8287 - precision: 0.8125 - auc: 0.8938 - recall: 0.7872 - val_loss: 0.3523 - val_accuracy: 0.8353 - val_precision: 0.8125 - val_auc: 0.8938 - val_recall: 0.7873\n",
            "Epoch 284/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3609 - accuracy: 0.8299 - precision: 0.8126 - auc: 0.8939 - recall: 0.7873 - val_loss: 0.3519 - val_accuracy: 0.8327 - val_precision: 0.8127 - val_auc: 0.8939 - val_recall: 0.7874\n",
            "Epoch 285/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3600 - accuracy: 0.8306 - precision: 0.8127 - auc: 0.8940 - recall: 0.7874 - val_loss: 0.3505 - val_accuracy: 0.8349 - val_precision: 0.8128 - val_auc: 0.8940 - val_recall: 0.7875\n",
            "Epoch 286/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3633 - accuracy: 0.8292 - precision: 0.8128 - auc: 0.8941 - recall: 0.7875 - val_loss: 0.3497 - val_accuracy: 0.8372 - val_precision: 0.8129 - val_auc: 0.8941 - val_recall: 0.7876\n",
            "Epoch 287/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3610 - accuracy: 0.8308 - precision: 0.8129 - auc: 0.8942 - recall: 0.7876 - val_loss: 0.3490 - val_accuracy: 0.8385 - val_precision: 0.8130 - val_auc: 0.8942 - val_recall: 0.7877\n",
            "Epoch 288/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3618 - accuracy: 0.8292 - precision: 0.8130 - auc: 0.8943 - recall: 0.7877 - val_loss: 0.3534 - val_accuracy: 0.8341 - val_precision: 0.8131 - val_auc: 0.8943 - val_recall: 0.7878\n",
            "Epoch 289/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3623 - accuracy: 0.8298 - precision: 0.8131 - auc: 0.8943 - recall: 0.7878 - val_loss: 0.3503 - val_accuracy: 0.8346 - val_precision: 0.8132 - val_auc: 0.8944 - val_recall: 0.7879\n",
            "Epoch 290/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3613 - accuracy: 0.8296 - precision: 0.8133 - auc: 0.8944 - recall: 0.7879 - val_loss: 0.3542 - val_accuracy: 0.8319 - val_precision: 0.8133 - val_auc: 0.8945 - val_recall: 0.7880\n",
            "Epoch 291/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3611 - accuracy: 0.8301 - precision: 0.8133 - auc: 0.8945 - recall: 0.7881 - val_loss: 0.3514 - val_accuracy: 0.8356 - val_precision: 0.8134 - val_auc: 0.8946 - val_recall: 0.7881\n",
            "Epoch 292/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3614 - accuracy: 0.8305 - precision: 0.8135 - auc: 0.8946 - recall: 0.7882 - val_loss: 0.3515 - val_accuracy: 0.8354 - val_precision: 0.8135 - val_auc: 0.8946 - val_recall: 0.7882\n",
            "Epoch 293/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3598 - accuracy: 0.8304 - precision: 0.8136 - auc: 0.8947 - recall: 0.7883 - val_loss: 0.3538 - val_accuracy: 0.8332 - val_precision: 0.8136 - val_auc: 0.8947 - val_recall: 0.7883\n",
            "Epoch 294/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3607 - accuracy: 0.8318 - precision: 0.8137 - auc: 0.8948 - recall: 0.7884 - val_loss: 0.3503 - val_accuracy: 0.8326 - val_precision: 0.8137 - val_auc: 0.8948 - val_recall: 0.7884\n",
            "Epoch 295/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3620 - accuracy: 0.8299 - precision: 0.8138 - auc: 0.8949 - recall: 0.7885 - val_loss: 0.3523 - val_accuracy: 0.8356 - val_precision: 0.8138 - val_auc: 0.8949 - val_recall: 0.7885\n",
            "Epoch 296/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3627 - accuracy: 0.8296 - precision: 0.8139 - auc: 0.8949 - recall: 0.7886 - val_loss: 0.3484 - val_accuracy: 0.8380 - val_precision: 0.8139 - val_auc: 0.8950 - val_recall: 0.7886\n",
            "Epoch 297/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3621 - accuracy: 0.8303 - precision: 0.8140 - auc: 0.8950 - recall: 0.7887 - val_loss: 0.3515 - val_accuracy: 0.8349 - val_precision: 0.8140 - val_auc: 0.8951 - val_recall: 0.7887\n",
            "Epoch 298/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3627 - accuracy: 0.8300 - precision: 0.8141 - auc: 0.8951 - recall: 0.7888 - val_loss: 0.3498 - val_accuracy: 0.8345 - val_precision: 0.8141 - val_auc: 0.8952 - val_recall: 0.7888\n",
            "Epoch 299/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3605 - accuracy: 0.8316 - precision: 0.8142 - auc: 0.8952 - recall: 0.7889 - val_loss: 0.3497 - val_accuracy: 0.8329 - val_precision: 0.8142 - val_auc: 0.8952 - val_recall: 0.7889\n",
            "Epoch 300/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3620 - accuracy: 0.8304 - precision: 0.8142 - auc: 0.8953 - recall: 0.7890 - val_loss: 0.3529 - val_accuracy: 0.8329 - val_precision: 0.8143 - val_auc: 0.8953 - val_recall: 0.7890\n",
            "Epoch 301/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3605 - accuracy: 0.8313 - precision: 0.8144 - auc: 0.8954 - recall: 0.7890 - val_loss: 0.3500 - val_accuracy: 0.8357 - val_precision: 0.8144 - val_auc: 0.8954 - val_recall: 0.7891\n",
            "Epoch 302/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3599 - accuracy: 0.8316 - precision: 0.8144 - auc: 0.8955 - recall: 0.7892 - val_loss: 0.3492 - val_accuracy: 0.8369 - val_precision: 0.8145 - val_auc: 0.8955 - val_recall: 0.7892\n",
            "Epoch 303/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3581 - accuracy: 0.8315 - precision: 0.8145 - auc: 0.8955 - recall: 0.7893 - val_loss: 0.3454 - val_accuracy: 0.8367 - val_precision: 0.8146 - val_auc: 0.8956 - val_recall: 0.7893\n",
            "Epoch 304/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3598 - accuracy: 0.8308 - precision: 0.8146 - auc: 0.8956 - recall: 0.7893 - val_loss: 0.3506 - val_accuracy: 0.8333 - val_precision: 0.8147 - val_auc: 0.8957 - val_recall: 0.7894\n",
            "Epoch 305/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3604 - accuracy: 0.8316 - precision: 0.8147 - auc: 0.8957 - recall: 0.7895 - val_loss: 0.3483 - val_accuracy: 0.8367 - val_precision: 0.8148 - val_auc: 0.8957 - val_recall: 0.7895\n",
            "Epoch 306/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3621 - accuracy: 0.8294 - precision: 0.8148 - auc: 0.8958 - recall: 0.7895 - val_loss: 0.3500 - val_accuracy: 0.8336 - val_precision: 0.8149 - val_auc: 0.8958 - val_recall: 0.7896\n",
            "Epoch 307/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3615 - accuracy: 0.8307 - precision: 0.8149 - auc: 0.8959 - recall: 0.7896 - val_loss: 0.3502 - val_accuracy: 0.8358 - val_precision: 0.8150 - val_auc: 0.8959 - val_recall: 0.7897\n",
            "Epoch 308/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3600 - accuracy: 0.8306 - precision: 0.8150 - auc: 0.8959 - recall: 0.7897 - val_loss: 0.3474 - val_accuracy: 0.8345 - val_precision: 0.8151 - val_auc: 0.8960 - val_recall: 0.7898\n",
            "Epoch 309/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3612 - accuracy: 0.8309 - precision: 0.8151 - auc: 0.8960 - recall: 0.7898 - val_loss: 0.3488 - val_accuracy: 0.8349 - val_precision: 0.8152 - val_auc: 0.8961 - val_recall: 0.7899\n",
            "Epoch 310/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3608 - accuracy: 0.8296 - precision: 0.8152 - auc: 0.8961 - recall: 0.7899 - val_loss: 0.3487 - val_accuracy: 0.8341 - val_precision: 0.8152 - val_auc: 0.8961 - val_recall: 0.7900\n",
            "Epoch 311/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3590 - accuracy: 0.8324 - precision: 0.8153 - auc: 0.8962 - recall: 0.7900 - val_loss: 0.3475 - val_accuracy: 0.8328 - val_precision: 0.8153 - val_auc: 0.8962 - val_recall: 0.7901\n",
            "Epoch 312/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3604 - accuracy: 0.8304 - precision: 0.8154 - auc: 0.8963 - recall: 0.7901 - val_loss: 0.3489 - val_accuracy: 0.8349 - val_precision: 0.8154 - val_auc: 0.8963 - val_recall: 0.7902\n",
            "Epoch 313/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3591 - accuracy: 0.8310 - precision: 0.8155 - auc: 0.8963 - recall: 0.7902 - val_loss: 0.3531 - val_accuracy: 0.8303 - val_precision: 0.8155 - val_auc: 0.8964 - val_recall: 0.7902\n",
            "Epoch 314/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3601 - accuracy: 0.8296 - precision: 0.8156 - auc: 0.8964 - recall: 0.7903 - val_loss: 0.3509 - val_accuracy: 0.8327 - val_precision: 0.8156 - val_auc: 0.8965 - val_recall: 0.7903\n",
            "Epoch 315/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3600 - accuracy: 0.8301 - precision: 0.8157 - auc: 0.8965 - recall: 0.7904 - val_loss: 0.3483 - val_accuracy: 0.8350 - val_precision: 0.8157 - val_auc: 0.8965 - val_recall: 0.7904\n",
            "Epoch 316/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3590 - accuracy: 0.8321 - precision: 0.8157 - auc: 0.8966 - recall: 0.7904 - val_loss: 0.3461 - val_accuracy: 0.8373 - val_precision: 0.8158 - val_auc: 0.8966 - val_recall: 0.7905\n",
            "Epoch 317/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3594 - accuracy: 0.8314 - precision: 0.8158 - auc: 0.8967 - recall: 0.7905 - val_loss: 0.3458 - val_accuracy: 0.8374 - val_precision: 0.8159 - val_auc: 0.8967 - val_recall: 0.7906\n",
            "Epoch 318/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3605 - accuracy: 0.8304 - precision: 0.8159 - auc: 0.8967 - recall: 0.7906 - val_loss: 0.3480 - val_accuracy: 0.8340 - val_precision: 0.8160 - val_auc: 0.8968 - val_recall: 0.7907\n",
            "Epoch 319/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3576 - accuracy: 0.8322 - precision: 0.8160 - auc: 0.8968 - recall: 0.7907 - val_loss: 0.3474 - val_accuracy: 0.8365 - val_precision: 0.8161 - val_auc: 0.8968 - val_recall: 0.7907\n",
            "Epoch 320/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3582 - accuracy: 0.8321 - precision: 0.8161 - auc: 0.8969 - recall: 0.7908 - val_loss: 0.3455 - val_accuracy: 0.8374 - val_precision: 0.8162 - val_auc: 0.8969 - val_recall: 0.7908\n",
            "Epoch 321/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3590 - accuracy: 0.8320 - precision: 0.8162 - auc: 0.8970 - recall: 0.7909 - val_loss: 0.3487 - val_accuracy: 0.8355 - val_precision: 0.8162 - val_auc: 0.8970 - val_recall: 0.7909\n",
            "Epoch 322/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3603 - accuracy: 0.8308 - precision: 0.8163 - auc: 0.8970 - recall: 0.7910 - val_loss: 0.3485 - val_accuracy: 0.8368 - val_precision: 0.8163 - val_auc: 0.8971 - val_recall: 0.7910\n",
            "Epoch 323/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3582 - accuracy: 0.8319 - precision: 0.8164 - auc: 0.8971 - recall: 0.7911 - val_loss: 0.3488 - val_accuracy: 0.8349 - val_precision: 0.8164 - val_auc: 0.8971 - val_recall: 0.7911\n",
            "Epoch 324/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3586 - accuracy: 0.8324 - precision: 0.8164 - auc: 0.8972 - recall: 0.7912 - val_loss: 0.3473 - val_accuracy: 0.8357 - val_precision: 0.8165 - val_auc: 0.8972 - val_recall: 0.7912\n",
            "Epoch 325/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3601 - accuracy: 0.8323 - precision: 0.8165 - auc: 0.8973 - recall: 0.7913 - val_loss: 0.3468 - val_accuracy: 0.8366 - val_precision: 0.8166 - val_auc: 0.8973 - val_recall: 0.7913\n",
            "Epoch 326/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3601 - accuracy: 0.8314 - precision: 0.8166 - auc: 0.8973 - recall: 0.7914 - val_loss: 0.3467 - val_accuracy: 0.8353 - val_precision: 0.8167 - val_auc: 0.8974 - val_recall: 0.7914\n",
            "Epoch 327/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3593 - accuracy: 0.8322 - precision: 0.8167 - auc: 0.8974 - recall: 0.7914 - val_loss: 0.3447 - val_accuracy: 0.8365 - val_precision: 0.8167 - val_auc: 0.8974 - val_recall: 0.7915\n",
            "Epoch 328/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3588 - accuracy: 0.8315 - precision: 0.8168 - auc: 0.8975 - recall: 0.7915 - val_loss: 0.3461 - val_accuracy: 0.8349 - val_precision: 0.8168 - val_auc: 0.8975 - val_recall: 0.7916\n",
            "Epoch 329/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3591 - accuracy: 0.8315 - precision: 0.8169 - auc: 0.8976 - recall: 0.7916 - val_loss: 0.3465 - val_accuracy: 0.8348 - val_precision: 0.8169 - val_auc: 0.8976 - val_recall: 0.7917\n",
            "Epoch 330/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3585 - accuracy: 0.8329 - precision: 0.8170 - auc: 0.8976 - recall: 0.7917 - val_loss: 0.3451 - val_accuracy: 0.8369 - val_precision: 0.8170 - val_auc: 0.8977 - val_recall: 0.7917\n",
            "Epoch 331/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3577 - accuracy: 0.8319 - precision: 0.8171 - auc: 0.8977 - recall: 0.7918 - val_loss: 0.3452 - val_accuracy: 0.8366 - val_precision: 0.8171 - val_auc: 0.8977 - val_recall: 0.7918\n",
            "Epoch 332/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3553 - accuracy: 0.8345 - precision: 0.8171 - auc: 0.8978 - recall: 0.7919 - val_loss: 0.3445 - val_accuracy: 0.8392 - val_precision: 0.8172 - val_auc: 0.8978 - val_recall: 0.7919\n",
            "Epoch 333/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3565 - accuracy: 0.8333 - precision: 0.8172 - auc: 0.8978 - recall: 0.7920 - val_loss: 0.3458 - val_accuracy: 0.8390 - val_precision: 0.8173 - val_auc: 0.8979 - val_recall: 0.7920\n",
            "Epoch 334/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3583 - accuracy: 0.8322 - precision: 0.8173 - auc: 0.8979 - recall: 0.7921 - val_loss: 0.3451 - val_accuracy: 0.8392 - val_precision: 0.8173 - val_auc: 0.8980 - val_recall: 0.7921\n",
            "Epoch 335/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3565 - accuracy: 0.8333 - precision: 0.8174 - auc: 0.8980 - recall: 0.7922 - val_loss: 0.3429 - val_accuracy: 0.8374 - val_precision: 0.8174 - val_auc: 0.8980 - val_recall: 0.7922\n",
            "Epoch 336/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3568 - accuracy: 0.8330 - precision: 0.8175 - auc: 0.8981 - recall: 0.7922 - val_loss: 0.3442 - val_accuracy: 0.8387 - val_precision: 0.8175 - val_auc: 0.8981 - val_recall: 0.7923\n",
            "Epoch 337/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3553 - accuracy: 0.8344 - precision: 0.8175 - auc: 0.8981 - recall: 0.7923 - val_loss: 0.3482 - val_accuracy: 0.8341 - val_precision: 0.8176 - val_auc: 0.8982 - val_recall: 0.7924\n",
            "Epoch 338/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3577 - accuracy: 0.8333 - precision: 0.8176 - auc: 0.8982 - recall: 0.7924 - val_loss: 0.3470 - val_accuracy: 0.8361 - val_precision: 0.8177 - val_auc: 0.8982 - val_recall: 0.7925\n",
            "Epoch 339/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3590 - accuracy: 0.8319 - precision: 0.8177 - auc: 0.8983 - recall: 0.7925 - val_loss: 0.3441 - val_accuracy: 0.8392 - val_precision: 0.8177 - val_auc: 0.8983 - val_recall: 0.7926\n",
            "Epoch 340/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3561 - accuracy: 0.8333 - precision: 0.8178 - auc: 0.8984 - recall: 0.7926 - val_loss: 0.3429 - val_accuracy: 0.8389 - val_precision: 0.8178 - val_auc: 0.8984 - val_recall: 0.7926\n",
            "Epoch 341/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3547 - accuracy: 0.8338 - precision: 0.8179 - auc: 0.8984 - recall: 0.7927 - val_loss: 0.3425 - val_accuracy: 0.8385 - val_precision: 0.8179 - val_auc: 0.8985 - val_recall: 0.7927\n",
            "Epoch 342/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3570 - accuracy: 0.8332 - precision: 0.8180 - auc: 0.8985 - recall: 0.7928 - val_loss: 0.3450 - val_accuracy: 0.8365 - val_precision: 0.8180 - val_auc: 0.8985 - val_recall: 0.7928\n",
            "Epoch 343/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3569 - accuracy: 0.8344 - precision: 0.8180 - auc: 0.8986 - recall: 0.7928 - val_loss: 0.3440 - val_accuracy: 0.8366 - val_precision: 0.8181 - val_auc: 0.8986 - val_recall: 0.7929\n",
            "Epoch 344/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3585 - accuracy: 0.8334 - precision: 0.8181 - auc: 0.8986 - recall: 0.7929 - val_loss: 0.3445 - val_accuracy: 0.8359 - val_precision: 0.8182 - val_auc: 0.8987 - val_recall: 0.7930\n",
            "Epoch 345/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3575 - accuracy: 0.8323 - precision: 0.8182 - auc: 0.8987 - recall: 0.7930 - val_loss: 0.3422 - val_accuracy: 0.8373 - val_precision: 0.8182 - val_auc: 0.8987 - val_recall: 0.7930\n",
            "Epoch 346/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3566 - accuracy: 0.8334 - precision: 0.8183 - auc: 0.8988 - recall: 0.7931 - val_loss: 0.3471 - val_accuracy: 0.8362 - val_precision: 0.8183 - val_auc: 0.8988 - val_recall: 0.7931\n",
            "Epoch 347/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3568 - accuracy: 0.8332 - precision: 0.8184 - auc: 0.8988 - recall: 0.7932 - val_loss: 0.3439 - val_accuracy: 0.8374 - val_precision: 0.8184 - val_auc: 0.8989 - val_recall: 0.7932\n",
            "Epoch 348/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3566 - accuracy: 0.8330 - precision: 0.8184 - auc: 0.8989 - recall: 0.7933 - val_loss: 0.3442 - val_accuracy: 0.8368 - val_precision: 0.8185 - val_auc: 0.8989 - val_recall: 0.7933\n",
            "Epoch 349/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3538 - accuracy: 0.8345 - precision: 0.8185 - auc: 0.8990 - recall: 0.7933 - val_loss: 0.3463 - val_accuracy: 0.8353 - val_precision: 0.8186 - val_auc: 0.8990 - val_recall: 0.7934\n",
            "Epoch 350/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3549 - accuracy: 0.8355 - precision: 0.8186 - auc: 0.8990 - recall: 0.7934 - val_loss: 0.3463 - val_accuracy: 0.8375 - val_precision: 0.8186 - val_auc: 0.8991 - val_recall: 0.7935\n",
            "Epoch 351/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3549 - accuracy: 0.8339 - precision: 0.8187 - auc: 0.8991 - recall: 0.7935 - val_loss: 0.3435 - val_accuracy: 0.8384 - val_precision: 0.8187 - val_auc: 0.8992 - val_recall: 0.7935\n",
            "Epoch 352/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3546 - accuracy: 0.8349 - precision: 0.8188 - auc: 0.8992 - recall: 0.7936 - val_loss: 0.3390 - val_accuracy: 0.8396 - val_precision: 0.8188 - val_auc: 0.8992 - val_recall: 0.7936\n",
            "Epoch 353/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3567 - accuracy: 0.8338 - precision: 0.8189 - auc: 0.8993 - recall: 0.7937 - val_loss: 0.3410 - val_accuracy: 0.8399 - val_precision: 0.8189 - val_auc: 0.8993 - val_recall: 0.7937\n",
            "Epoch 354/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3553 - accuracy: 0.8341 - precision: 0.8189 - auc: 0.8993 - recall: 0.7937 - val_loss: 0.3407 - val_accuracy: 0.8404 - val_precision: 0.8190 - val_auc: 0.8994 - val_recall: 0.7938\n",
            "Epoch 355/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3565 - accuracy: 0.8345 - precision: 0.8190 - auc: 0.8994 - recall: 0.7938 - val_loss: 0.3427 - val_accuracy: 0.8374 - val_precision: 0.8191 - val_auc: 0.8994 - val_recall: 0.7939\n",
            "Epoch 356/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3574 - accuracy: 0.8321 - precision: 0.8191 - auc: 0.8995 - recall: 0.7939 - val_loss: 0.3428 - val_accuracy: 0.8400 - val_precision: 0.8191 - val_auc: 0.8995 - val_recall: 0.7939\n",
            "Epoch 357/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3567 - accuracy: 0.8335 - precision: 0.8192 - auc: 0.8995 - recall: 0.7940 - val_loss: 0.3448 - val_accuracy: 0.8384 - val_precision: 0.8192 - val_auc: 0.8995 - val_recall: 0.7940\n",
            "Epoch 358/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3562 - accuracy: 0.8333 - precision: 0.8193 - auc: 0.8996 - recall: 0.7940 - val_loss: 0.3429 - val_accuracy: 0.8374 - val_precision: 0.8193 - val_auc: 0.8996 - val_recall: 0.7941\n",
            "Epoch 359/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3559 - accuracy: 0.8339 - precision: 0.8193 - auc: 0.8996 - recall: 0.7941 - val_loss: 0.3462 - val_accuracy: 0.8381 - val_precision: 0.8194 - val_auc: 0.8997 - val_recall: 0.7941\n",
            "Epoch 360/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3557 - accuracy: 0.8337 - precision: 0.8194 - auc: 0.8997 - recall: 0.7942 - val_loss: 0.3418 - val_accuracy: 0.8385 - val_precision: 0.8194 - val_auc: 0.8997 - val_recall: 0.7942\n",
            "Epoch 361/500\n",
            "37725/37725 [==============================] - 42s 1ms/step - loss: 0.3539 - accuracy: 0.8348 - precision: 0.8195 - auc: 0.8998 - recall: 0.7943 - val_loss: 0.3408 - val_accuracy: 0.8402 - val_precision: 0.8195 - val_auc: 0.8998 - val_recall: 0.7943\n",
            "Epoch 362/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3542 - accuracy: 0.8350 - precision: 0.8196 - auc: 0.8998 - recall: 0.7943 - val_loss: 0.3433 - val_accuracy: 0.8366 - val_precision: 0.8196 - val_auc: 0.8999 - val_recall: 0.7944\n",
            "Epoch 363/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3532 - accuracy: 0.8360 - precision: 0.8196 - auc: 0.8999 - recall: 0.7944 - val_loss: 0.3404 - val_accuracy: 0.8396 - val_precision: 0.8197 - val_auc: 0.8999 - val_recall: 0.7945\n",
            "Epoch 364/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3527 - accuracy: 0.8349 - precision: 0.8197 - auc: 0.9000 - recall: 0.7945 - val_loss: 0.3431 - val_accuracy: 0.8396 - val_precision: 0.8198 - val_auc: 0.9000 - val_recall: 0.7945\n",
            "Epoch 365/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3562 - accuracy: 0.8342 - precision: 0.8198 - auc: 0.9000 - recall: 0.7946 - val_loss: 0.3398 - val_accuracy: 0.8417 - val_precision: 0.8198 - val_auc: 0.9001 - val_recall: 0.7946\n",
            "Epoch 366/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3525 - accuracy: 0.8359 - precision: 0.8199 - auc: 0.9001 - recall: 0.7947 - val_loss: 0.3389 - val_accuracy: 0.8395 - val_precision: 0.8199 - val_auc: 0.9001 - val_recall: 0.7947\n",
            "Epoch 367/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3553 - accuracy: 0.8340 - precision: 0.8199 - auc: 0.9002 - recall: 0.7947 - val_loss: 0.3436 - val_accuracy: 0.8374 - val_precision: 0.8200 - val_auc: 0.9002 - val_recall: 0.7948\n",
            "Epoch 368/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3528 - accuracy: 0.8353 - precision: 0.8200 - auc: 0.9002 - recall: 0.7948 - val_loss: 0.3429 - val_accuracy: 0.8387 - val_precision: 0.8201 - val_auc: 0.9003 - val_recall: 0.7948\n",
            "Epoch 369/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3560 - accuracy: 0.8337 - precision: 0.8201 - auc: 0.9003 - recall: 0.7949 - val_loss: 0.3420 - val_accuracy: 0.8388 - val_precision: 0.8201 - val_auc: 0.9003 - val_recall: 0.7949\n",
            "Epoch 370/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3528 - accuracy: 0.8358 - precision: 0.8202 - auc: 0.9004 - recall: 0.7950 - val_loss: 0.3431 - val_accuracy: 0.8382 - val_precision: 0.8202 - val_auc: 0.9004 - val_recall: 0.7950\n",
            "Epoch 371/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3564 - accuracy: 0.8334 - precision: 0.8202 - auc: 0.9004 - recall: 0.7950 - val_loss: 0.3473 - val_accuracy: 0.8380 - val_precision: 0.8203 - val_auc: 0.9004 - val_recall: 0.7951\n",
            "Epoch 372/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3550 - accuracy: 0.8343 - precision: 0.8203 - auc: 0.9005 - recall: 0.7951 - val_loss: 0.3420 - val_accuracy: 0.8387 - val_precision: 0.8203 - val_auc: 0.9005 - val_recall: 0.7951\n",
            "Epoch 373/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3564 - accuracy: 0.8337 - precision: 0.8204 - auc: 0.9005 - recall: 0.7952 - val_loss: 0.3441 - val_accuracy: 0.8385 - val_precision: 0.8204 - val_auc: 0.9006 - val_recall: 0.7952\n",
            "Epoch 374/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3557 - accuracy: 0.8334 - precision: 0.8205 - auc: 0.9006 - recall: 0.7952 - val_loss: 0.3445 - val_accuracy: 0.8369 - val_precision: 0.8205 - val_auc: 0.9006 - val_recall: 0.7953\n",
            "Epoch 375/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3556 - accuracy: 0.8340 - precision: 0.8205 - auc: 0.9007 - recall: 0.7953 - val_loss: 0.3401 - val_accuracy: 0.8386 - val_precision: 0.8205 - val_auc: 0.9007 - val_recall: 0.7954\n",
            "Epoch 376/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3536 - accuracy: 0.8346 - precision: 0.8206 - auc: 0.9007 - recall: 0.7954 - val_loss: 0.3387 - val_accuracy: 0.8396 - val_precision: 0.8206 - val_auc: 0.9007 - val_recall: 0.7955\n",
            "Epoch 377/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3549 - accuracy: 0.8353 - precision: 0.8207 - auc: 0.9008 - recall: 0.7955 - val_loss: 0.3433 - val_accuracy: 0.8371 - val_precision: 0.8207 - val_auc: 0.9008 - val_recall: 0.7955\n",
            "Epoch 378/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3558 - accuracy: 0.8332 - precision: 0.8207 - auc: 0.9008 - recall: 0.7956 - val_loss: 0.3422 - val_accuracy: 0.8386 - val_precision: 0.8207 - val_auc: 0.9009 - val_recall: 0.7956\n",
            "Epoch 379/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3541 - accuracy: 0.8351 - precision: 0.8208 - auc: 0.9009 - recall: 0.7957 - val_loss: 0.3395 - val_accuracy: 0.8376 - val_precision: 0.8208 - val_auc: 0.9009 - val_recall: 0.7957\n",
            "Epoch 380/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3542 - accuracy: 0.8354 - precision: 0.8208 - auc: 0.9010 - recall: 0.7957 - val_loss: 0.3441 - val_accuracy: 0.8362 - val_precision: 0.8209 - val_auc: 0.9010 - val_recall: 0.7958\n",
            "Epoch 381/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3537 - accuracy: 0.8346 - precision: 0.8209 - auc: 0.9010 - recall: 0.7958 - val_loss: 0.3452 - val_accuracy: 0.8354 - val_precision: 0.8209 - val_auc: 0.9010 - val_recall: 0.7958\n",
            "Epoch 382/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3532 - accuracy: 0.8354 - precision: 0.8210 - auc: 0.9011 - recall: 0.7959 - val_loss: 0.3402 - val_accuracy: 0.8395 - val_precision: 0.8210 - val_auc: 0.9011 - val_recall: 0.7959\n",
            "Epoch 383/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3527 - accuracy: 0.8350 - precision: 0.8210 - auc: 0.9011 - recall: 0.7959 - val_loss: 0.3438 - val_accuracy: 0.8385 - val_precision: 0.8211 - val_auc: 0.9012 - val_recall: 0.7960\n",
            "Epoch 384/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3552 - accuracy: 0.8335 - precision: 0.8211 - auc: 0.9012 - recall: 0.7960 - val_loss: 0.3410 - val_accuracy: 0.8389 - val_precision: 0.8211 - val_auc: 0.9012 - val_recall: 0.7961\n",
            "Epoch 385/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3544 - accuracy: 0.8346 - precision: 0.8212 - auc: 0.9012 - recall: 0.7961 - val_loss: 0.3417 - val_accuracy: 0.8368 - val_precision: 0.8212 - val_auc: 0.9013 - val_recall: 0.7961\n",
            "Epoch 386/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3525 - accuracy: 0.8357 - precision: 0.8212 - auc: 0.9013 - recall: 0.7962 - val_loss: 0.3411 - val_accuracy: 0.8398 - val_precision: 0.8213 - val_auc: 0.9013 - val_recall: 0.7962\n",
            "Epoch 387/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3505 - accuracy: 0.8371 - precision: 0.8213 - auc: 0.9014 - recall: 0.7963 - val_loss: 0.3414 - val_accuracy: 0.8396 - val_precision: 0.8213 - val_auc: 0.9014 - val_recall: 0.7963\n",
            "Epoch 388/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3545 - accuracy: 0.8356 - precision: 0.8214 - auc: 0.9014 - recall: 0.7963 - val_loss: 0.3425 - val_accuracy: 0.8384 - val_precision: 0.8214 - val_auc: 0.9015 - val_recall: 0.7964\n",
            "Epoch 389/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3537 - accuracy: 0.8358 - precision: 0.8214 - auc: 0.9015 - recall: 0.7964 - val_loss: 0.3425 - val_accuracy: 0.8393 - val_precision: 0.8215 - val_auc: 0.9015 - val_recall: 0.7965\n",
            "Epoch 390/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3511 - accuracy: 0.8365 - precision: 0.8215 - auc: 0.9015 - recall: 0.7965 - val_loss: 0.3438 - val_accuracy: 0.8389 - val_precision: 0.8215 - val_auc: 0.9016 - val_recall: 0.7965\n",
            "Epoch 391/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3526 - accuracy: 0.8361 - precision: 0.8216 - auc: 0.9016 - recall: 0.7966 - val_loss: 0.3404 - val_accuracy: 0.8408 - val_precision: 0.8216 - val_auc: 0.9016 - val_recall: 0.7966\n",
            "Epoch 392/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3524 - accuracy: 0.8355 - precision: 0.8216 - auc: 0.9017 - recall: 0.7966 - val_loss: 0.3408 - val_accuracy: 0.8392 - val_precision: 0.8217 - val_auc: 0.9017 - val_recall: 0.7967\n",
            "Epoch 393/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3512 - accuracy: 0.8364 - precision: 0.8217 - auc: 0.9017 - recall: 0.7967 - val_loss: 0.3411 - val_accuracy: 0.8396 - val_precision: 0.8217 - val_auc: 0.9018 - val_recall: 0.7967\n",
            "Epoch 394/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3516 - accuracy: 0.8372 - precision: 0.8218 - auc: 0.9018 - recall: 0.7968 - val_loss: 0.3428 - val_accuracy: 0.8383 - val_precision: 0.8218 - val_auc: 0.9018 - val_recall: 0.7968\n",
            "Epoch 395/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3528 - accuracy: 0.8363 - precision: 0.8218 - auc: 0.9018 - recall: 0.7969 - val_loss: 0.3394 - val_accuracy: 0.8386 - val_precision: 0.8219 - val_auc: 0.9019 - val_recall: 0.7969\n",
            "Epoch 396/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3528 - accuracy: 0.8360 - precision: 0.8219 - auc: 0.9019 - recall: 0.7969 - val_loss: 0.3404 - val_accuracy: 0.8394 - val_precision: 0.8219 - val_auc: 0.9019 - val_recall: 0.7970\n",
            "Epoch 397/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3499 - accuracy: 0.8377 - precision: 0.8219 - auc: 0.9020 - recall: 0.7970 - val_loss: 0.3419 - val_accuracy: 0.8371 - val_precision: 0.8220 - val_auc: 0.9020 - val_recall: 0.7971\n",
            "Epoch 398/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3513 - accuracy: 0.8356 - precision: 0.8220 - auc: 0.9020 - recall: 0.7971 - val_loss: 0.3403 - val_accuracy: 0.8402 - val_precision: 0.8220 - val_auc: 0.9020 - val_recall: 0.7971\n",
            "Epoch 399/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3522 - accuracy: 0.8360 - precision: 0.8221 - auc: 0.9021 - recall: 0.7972 - val_loss: 0.3399 - val_accuracy: 0.8385 - val_precision: 0.8221 - val_auc: 0.9021 - val_recall: 0.7972\n",
            "Epoch 400/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3529 - accuracy: 0.8360 - precision: 0.8221 - auc: 0.9021 - recall: 0.7972 - val_loss: 0.3399 - val_accuracy: 0.8379 - val_precision: 0.8222 - val_auc: 0.9022 - val_recall: 0.7973\n",
            "Epoch 401/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3518 - accuracy: 0.8362 - precision: 0.8222 - auc: 0.9022 - recall: 0.7973 - val_loss: 0.3396 - val_accuracy: 0.8410 - val_precision: 0.8222 - val_auc: 0.9022 - val_recall: 0.7973\n",
            "Epoch 402/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3520 - accuracy: 0.8365 - precision: 0.8223 - auc: 0.9022 - recall: 0.7974 - val_loss: 0.3426 - val_accuracy: 0.8397 - val_precision: 0.8223 - val_auc: 0.9023 - val_recall: 0.7974\n",
            "Epoch 403/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3534 - accuracy: 0.8360 - precision: 0.8223 - auc: 0.9023 - recall: 0.7975 - val_loss: 0.3403 - val_accuracy: 0.8408 - val_precision: 0.8224 - val_auc: 0.9023 - val_recall: 0.7975\n",
            "Epoch 404/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3506 - accuracy: 0.8370 - precision: 0.8224 - auc: 0.9023 - recall: 0.7975 - val_loss: 0.3423 - val_accuracy: 0.8392 - val_precision: 0.8224 - val_auc: 0.9024 - val_recall: 0.7976\n",
            "Epoch 405/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3521 - accuracy: 0.8375 - precision: 0.8225 - auc: 0.9024 - recall: 0.7976 - val_loss: 0.3423 - val_accuracy: 0.8410 - val_precision: 0.8225 - val_auc: 0.9024 - val_recall: 0.7976\n",
            "Epoch 406/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3496 - accuracy: 0.8370 - precision: 0.8225 - auc: 0.9025 - recall: 0.7977 - val_loss: 0.3413 - val_accuracy: 0.8399 - val_precision: 0.8226 - val_auc: 0.9025 - val_recall: 0.7977\n",
            "Epoch 407/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3508 - accuracy: 0.8369 - precision: 0.8226 - auc: 0.9025 - recall: 0.7977 - val_loss: 0.3407 - val_accuracy: 0.8411 - val_precision: 0.8226 - val_auc: 0.9025 - val_recall: 0.7978\n",
            "Epoch 408/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3519 - accuracy: 0.8353 - precision: 0.8227 - auc: 0.9026 - recall: 0.7978 - val_loss: 0.3429 - val_accuracy: 0.8396 - val_precision: 0.8227 - val_auc: 0.9026 - val_recall: 0.7978\n",
            "Epoch 409/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3511 - accuracy: 0.8372 - precision: 0.8227 - auc: 0.9026 - recall: 0.7979 - val_loss: 0.3416 - val_accuracy: 0.8399 - val_precision: 0.8228 - val_auc: 0.9027 - val_recall: 0.7979\n",
            "Epoch 410/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3519 - accuracy: 0.8365 - precision: 0.8228 - auc: 0.9027 - recall: 0.7979 - val_loss: 0.3431 - val_accuracy: 0.8406 - val_precision: 0.8228 - val_auc: 0.9027 - val_recall: 0.7980\n",
            "Epoch 411/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3524 - accuracy: 0.8354 - precision: 0.8229 - auc: 0.9027 - recall: 0.7980 - val_loss: 0.3418 - val_accuracy: 0.8410 - val_precision: 0.8229 - val_auc: 0.9028 - val_recall: 0.7980\n",
            "Epoch 412/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3503 - accuracy: 0.8364 - precision: 0.8229 - auc: 0.9028 - recall: 0.7981 - val_loss: 0.3418 - val_accuracy: 0.8389 - val_precision: 0.8229 - val_auc: 0.9028 - val_recall: 0.7981\n",
            "Epoch 413/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3518 - accuracy: 0.8369 - precision: 0.8230 - auc: 0.9028 - recall: 0.7981 - val_loss: 0.3447 - val_accuracy: 0.8400 - val_precision: 0.8230 - val_auc: 0.9029 - val_recall: 0.7982\n",
            "Epoch 414/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3513 - accuracy: 0.8370 - precision: 0.8230 - auc: 0.9029 - recall: 0.7982 - val_loss: 0.3428 - val_accuracy: 0.8391 - val_precision: 0.8231 - val_auc: 0.9029 - val_recall: 0.7982\n",
            "Epoch 415/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3536 - accuracy: 0.8356 - precision: 0.8231 - auc: 0.9029 - recall: 0.7983 - val_loss: 0.3437 - val_accuracy: 0.8395 - val_precision: 0.8231 - val_auc: 0.9030 - val_recall: 0.7983\n",
            "Epoch 416/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3511 - accuracy: 0.8365 - precision: 0.8232 - auc: 0.9030 - recall: 0.7983 - val_loss: 0.3430 - val_accuracy: 0.8385 - val_precision: 0.8232 - val_auc: 0.9030 - val_recall: 0.7983\n",
            "Epoch 417/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3489 - accuracy: 0.8387 - precision: 0.8232 - auc: 0.9031 - recall: 0.7984 - val_loss: 0.3405 - val_accuracy: 0.8385 - val_precision: 0.8233 - val_auc: 0.9031 - val_recall: 0.7984\n",
            "Epoch 418/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3529 - accuracy: 0.8364 - precision: 0.8233 - auc: 0.9031 - recall: 0.7984 - val_loss: 0.3432 - val_accuracy: 0.8384 - val_precision: 0.8233 - val_auc: 0.9031 - val_recall: 0.7985\n",
            "Epoch 419/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3497 - accuracy: 0.8376 - precision: 0.8233 - auc: 0.9031 - recall: 0.7985 - val_loss: 0.3402 - val_accuracy: 0.8404 - val_precision: 0.8234 - val_auc: 0.9032 - val_recall: 0.7985\n",
            "Epoch 420/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3526 - accuracy: 0.8351 - precision: 0.8234 - auc: 0.9032 - recall: 0.7986 - val_loss: 0.3408 - val_accuracy: 0.8380 - val_precision: 0.8234 - val_auc: 0.9032 - val_recall: 0.7986\n",
            "Epoch 421/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3512 - accuracy: 0.8378 - precision: 0.8235 - auc: 0.9033 - recall: 0.7986 - val_loss: 0.3428 - val_accuracy: 0.8356 - val_precision: 0.8235 - val_auc: 0.9033 - val_recall: 0.7987\n",
            "Epoch 422/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3496 - accuracy: 0.8375 - precision: 0.8235 - auc: 0.9033 - recall: 0.7987 - val_loss: 0.3401 - val_accuracy: 0.8396 - val_precision: 0.8236 - val_auc: 0.9033 - val_recall: 0.7987\n",
            "Epoch 423/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3508 - accuracy: 0.8358 - precision: 0.8236 - auc: 0.9034 - recall: 0.7988 - val_loss: 0.3413 - val_accuracy: 0.8392 - val_precision: 0.8236 - val_auc: 0.9034 - val_recall: 0.7988\n",
            "Epoch 424/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3518 - accuracy: 0.8362 - precision: 0.8236 - auc: 0.9034 - recall: 0.7988 - val_loss: 0.3400 - val_accuracy: 0.8401 - val_precision: 0.8237 - val_auc: 0.9034 - val_recall: 0.7989\n",
            "Epoch 425/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3525 - accuracy: 0.8360 - precision: 0.8237 - auc: 0.9034 - recall: 0.7989 - val_loss: 0.3432 - val_accuracy: 0.8366 - val_precision: 0.8237 - val_auc: 0.9035 - val_recall: 0.7989\n",
            "Epoch 426/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3517 - accuracy: 0.8367 - precision: 0.8238 - auc: 0.9035 - recall: 0.7990 - val_loss: 0.3399 - val_accuracy: 0.8382 - val_precision: 0.8238 - val_auc: 0.9035 - val_recall: 0.7990\n",
            "Epoch 427/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3506 - accuracy: 0.8367 - precision: 0.8238 - auc: 0.9036 - recall: 0.7990 - val_loss: 0.3414 - val_accuracy: 0.8399 - val_precision: 0.8238 - val_auc: 0.9036 - val_recall: 0.7991\n",
            "Epoch 428/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3508 - accuracy: 0.8369 - precision: 0.8238 - auc: 0.9036 - recall: 0.7991 - val_loss: 0.3374 - val_accuracy: 0.8393 - val_precision: 0.8239 - val_auc: 0.9036 - val_recall: 0.7991\n",
            "Epoch 429/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3522 - accuracy: 0.8361 - precision: 0.8239 - auc: 0.9037 - recall: 0.7992 - val_loss: 0.3389 - val_accuracy: 0.8411 - val_precision: 0.8239 - val_auc: 0.9037 - val_recall: 0.7992\n",
            "Epoch 430/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3501 - accuracy: 0.8371 - precision: 0.8240 - auc: 0.9037 - recall: 0.7992 - val_loss: 0.3382 - val_accuracy: 0.8423 - val_precision: 0.8240 - val_auc: 0.9037 - val_recall: 0.7993\n",
            "Epoch 431/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3513 - accuracy: 0.8363 - precision: 0.8240 - auc: 0.9037 - recall: 0.7993 - val_loss: 0.3395 - val_accuracy: 0.8412 - val_precision: 0.8240 - val_auc: 0.9038 - val_recall: 0.7993\n",
            "Epoch 432/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3503 - accuracy: 0.8370 - precision: 0.8241 - auc: 0.9038 - recall: 0.7994 - val_loss: 0.3412 - val_accuracy: 0.8400 - val_precision: 0.8241 - val_auc: 0.9038 - val_recall: 0.7994\n",
            "Epoch 433/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3489 - accuracy: 0.8386 - precision: 0.8241 - auc: 0.9039 - recall: 0.7994 - val_loss: 0.3397 - val_accuracy: 0.8423 - val_precision: 0.8241 - val_auc: 0.9039 - val_recall: 0.7995\n",
            "Epoch 434/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3512 - accuracy: 0.8356 - precision: 0.8242 - auc: 0.9039 - recall: 0.7995 - val_loss: 0.3393 - val_accuracy: 0.8416 - val_precision: 0.8242 - val_auc: 0.9039 - val_recall: 0.7996\n",
            "Epoch 435/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3492 - accuracy: 0.8371 - precision: 0.8242 - auc: 0.9040 - recall: 0.7996 - val_loss: 0.3386 - val_accuracy: 0.8402 - val_precision: 0.8243 - val_auc: 0.9040 - val_recall: 0.7996\n",
            "Epoch 436/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3508 - accuracy: 0.8371 - precision: 0.8243 - auc: 0.9040 - recall: 0.7996 - val_loss: 0.3402 - val_accuracy: 0.8424 - val_precision: 0.8243 - val_auc: 0.9040 - val_recall: 0.7997\n",
            "Epoch 437/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3526 - accuracy: 0.8358 - precision: 0.8243 - auc: 0.9040 - recall: 0.7997 - val_loss: 0.3426 - val_accuracy: 0.8396 - val_precision: 0.8244 - val_auc: 0.9041 - val_recall: 0.7997\n",
            "Epoch 438/500\n",
            "37725/37725 [==============================] - 43s 1ms/step - loss: 0.3517 - accuracy: 0.8359 - precision: 0.8244 - auc: 0.9041 - recall: 0.7998 - val_loss: 0.3405 - val_accuracy: 0.8423 - val_precision: 0.8244 - val_auc: 0.9041 - val_recall: 0.7998\n",
            "Epoch 439/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3497 - accuracy: 0.8367 - precision: 0.8244 - auc: 0.9042 - recall: 0.7998 - val_loss: 0.3396 - val_accuracy: 0.8428 - val_precision: 0.8245 - val_auc: 0.9042 - val_recall: 0.7999\n",
            "Epoch 440/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3505 - accuracy: 0.8377 - precision: 0.8245 - auc: 0.9042 - recall: 0.7999 - val_loss: 0.3389 - val_accuracy: 0.8404 - val_precision: 0.8245 - val_auc: 0.9042 - val_recall: 0.7999\n",
            "Epoch 441/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3497 - accuracy: 0.8364 - precision: 0.8246 - auc: 0.9042 - recall: 0.8000 - val_loss: 0.3394 - val_accuracy: 0.8402 - val_precision: 0.8246 - val_auc: 0.9043 - val_recall: 0.8000\n",
            "Epoch 442/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3523 - accuracy: 0.8364 - precision: 0.8246 - auc: 0.9043 - recall: 0.8000 - val_loss: 0.3406 - val_accuracy: 0.8400 - val_precision: 0.8246 - val_auc: 0.9043 - val_recall: 0.8000\n",
            "Epoch 443/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3498 - accuracy: 0.8382 - precision: 0.8247 - auc: 0.9043 - recall: 0.8001 - val_loss: 0.3398 - val_accuracy: 0.8405 - val_precision: 0.8247 - val_auc: 0.9044 - val_recall: 0.8001\n",
            "Epoch 444/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3527 - accuracy: 0.8362 - precision: 0.8247 - auc: 0.9044 - recall: 0.8001 - val_loss: 0.3387 - val_accuracy: 0.8415 - val_precision: 0.8247 - val_auc: 0.9044 - val_recall: 0.8001\n",
            "Epoch 445/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3482 - accuracy: 0.8390 - precision: 0.8248 - auc: 0.9044 - recall: 0.8002 - val_loss: 0.3402 - val_accuracy: 0.8417 - val_precision: 0.8248 - val_auc: 0.9045 - val_recall: 0.8002\n",
            "Epoch 446/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3503 - accuracy: 0.8380 - precision: 0.8248 - auc: 0.9045 - recall: 0.8002 - val_loss: 0.3390 - val_accuracy: 0.8391 - val_precision: 0.8249 - val_auc: 0.9045 - val_recall: 0.8003\n",
            "Epoch 447/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3499 - accuracy: 0.8374 - precision: 0.8249 - auc: 0.9045 - recall: 0.8003 - val_loss: 0.3430 - val_accuracy: 0.8383 - val_precision: 0.8249 - val_auc: 0.9045 - val_recall: 0.8003\n",
            "Epoch 448/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3498 - accuracy: 0.8376 - precision: 0.8249 - auc: 0.9046 - recall: 0.8004 - val_loss: 0.3411 - val_accuracy: 0.8414 - val_precision: 0.8250 - val_auc: 0.9046 - val_recall: 0.8004\n",
            "Epoch 449/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3520 - accuracy: 0.8370 - precision: 0.8250 - auc: 0.9046 - recall: 0.8004 - val_loss: 0.3429 - val_accuracy: 0.8393 - val_precision: 0.8250 - val_auc: 0.9046 - val_recall: 0.8005\n",
            "Epoch 450/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3487 - accuracy: 0.8378 - precision: 0.8250 - auc: 0.9047 - recall: 0.8005 - val_loss: 0.3411 - val_accuracy: 0.8395 - val_precision: 0.8251 - val_auc: 0.9047 - val_recall: 0.8005\n",
            "Epoch 451/500\n",
            "37725/37725 [==============================] - 46s 1ms/step - loss: 0.3502 - accuracy: 0.8376 - precision: 0.8251 - auc: 0.9047 - recall: 0.8006 - val_loss: 0.3381 - val_accuracy: 0.8421 - val_precision: 0.8251 - val_auc: 0.9047 - val_recall: 0.8006\n",
            "Epoch 452/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3497 - accuracy: 0.8375 - precision: 0.8251 - auc: 0.9048 - recall: 0.8006 - val_loss: 0.3410 - val_accuracy: 0.8386 - val_precision: 0.8252 - val_auc: 0.9048 - val_recall: 0.8007\n",
            "Epoch 453/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3490 - accuracy: 0.8377 - precision: 0.8252 - auc: 0.9048 - recall: 0.8007 - val_loss: 0.3419 - val_accuracy: 0.8384 - val_precision: 0.8252 - val_auc: 0.9048 - val_recall: 0.8007\n",
            "Epoch 454/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3484 - accuracy: 0.8382 - precision: 0.8252 - auc: 0.9048 - recall: 0.8008 - val_loss: 0.3405 - val_accuracy: 0.8404 - val_precision: 0.8252 - val_auc: 0.9049 - val_recall: 0.8008\n",
            "Epoch 455/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3505 - accuracy: 0.8374 - precision: 0.8253 - auc: 0.9049 - recall: 0.8008 - val_loss: 0.3413 - val_accuracy: 0.8391 - val_precision: 0.8253 - val_auc: 0.9049 - val_recall: 0.8009\n",
            "Epoch 456/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3500 - accuracy: 0.8376 - precision: 0.8253 - auc: 0.9049 - recall: 0.8009 - val_loss: 0.3415 - val_accuracy: 0.8423 - val_precision: 0.8254 - val_auc: 0.9050 - val_recall: 0.8009\n",
            "Epoch 457/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3514 - accuracy: 0.8366 - precision: 0.8254 - auc: 0.9050 - recall: 0.8009 - val_loss: 0.3389 - val_accuracy: 0.8408 - val_precision: 0.8254 - val_auc: 0.9050 - val_recall: 0.8010\n",
            "Epoch 458/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3512 - accuracy: 0.8369 - precision: 0.8254 - auc: 0.9050 - recall: 0.8010 - val_loss: 0.3384 - val_accuracy: 0.8411 - val_precision: 0.8255 - val_auc: 0.9050 - val_recall: 0.8010\n",
            "Epoch 459/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3505 - accuracy: 0.8370 - precision: 0.8255 - auc: 0.9051 - recall: 0.8010 - val_loss: 0.3387 - val_accuracy: 0.8416 - val_precision: 0.8255 - val_auc: 0.9051 - val_recall: 0.8011\n",
            "Epoch 460/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3487 - accuracy: 0.8376 - precision: 0.8255 - auc: 0.9051 - recall: 0.8011 - val_loss: 0.3398 - val_accuracy: 0.8390 - val_precision: 0.8256 - val_auc: 0.9051 - val_recall: 0.8011\n",
            "Epoch 461/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3476 - accuracy: 0.8382 - precision: 0.8256 - auc: 0.9051 - recall: 0.8012 - val_loss: 0.3377 - val_accuracy: 0.8393 - val_precision: 0.8256 - val_auc: 0.9052 - val_recall: 0.8012\n",
            "Epoch 462/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3485 - accuracy: 0.8382 - precision: 0.8256 - auc: 0.9052 - recall: 0.8012 - val_loss: 0.3395 - val_accuracy: 0.8393 - val_precision: 0.8257 - val_auc: 0.9052 - val_recall: 0.8012\n",
            "Epoch 463/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3488 - accuracy: 0.8376 - precision: 0.8257 - auc: 0.9053 - recall: 0.8013 - val_loss: 0.3412 - val_accuracy: 0.8376 - val_precision: 0.8257 - val_auc: 0.9053 - val_recall: 0.8013\n",
            "Epoch 464/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3491 - accuracy: 0.8390 - precision: 0.8257 - auc: 0.9053 - recall: 0.8013 - val_loss: 0.3398 - val_accuracy: 0.8389 - val_precision: 0.8258 - val_auc: 0.9053 - val_recall: 0.8014\n",
            "Epoch 465/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3498 - accuracy: 0.8377 - precision: 0.8258 - auc: 0.9053 - recall: 0.8014 - val_loss: 0.3392 - val_accuracy: 0.8402 - val_precision: 0.8258 - val_auc: 0.9054 - val_recall: 0.8014\n",
            "Epoch 466/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3483 - accuracy: 0.8386 - precision: 0.8258 - auc: 0.9054 - recall: 0.8014 - val_loss: 0.3374 - val_accuracy: 0.8420 - val_precision: 0.8259 - val_auc: 0.9054 - val_recall: 0.8015\n",
            "Epoch 467/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3508 - accuracy: 0.8377 - precision: 0.8259 - auc: 0.9054 - recall: 0.8015 - val_loss: 0.3388 - val_accuracy: 0.8391 - val_precision: 0.8259 - val_auc: 0.9054 - val_recall: 0.8015\n",
            "Epoch 468/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3482 - accuracy: 0.8382 - precision: 0.8259 - auc: 0.9055 - recall: 0.8016 - val_loss: 0.3379 - val_accuracy: 0.8410 - val_precision: 0.8260 - val_auc: 0.9055 - val_recall: 0.8016\n",
            "Epoch 469/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3481 - accuracy: 0.8377 - precision: 0.8260 - auc: 0.9055 - recall: 0.8016 - val_loss: 0.3367 - val_accuracy: 0.8419 - val_precision: 0.8260 - val_auc: 0.9055 - val_recall: 0.8016\n",
            "Epoch 470/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3491 - accuracy: 0.8381 - precision: 0.8260 - auc: 0.9056 - recall: 0.8017 - val_loss: 0.3397 - val_accuracy: 0.8408 - val_precision: 0.8261 - val_auc: 0.9056 - val_recall: 0.8017\n",
            "Epoch 471/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3490 - accuracy: 0.8379 - precision: 0.8261 - auc: 0.9056 - recall: 0.8017 - val_loss: 0.3408 - val_accuracy: 0.8405 - val_precision: 0.8261 - val_auc: 0.9056 - val_recall: 0.8018\n",
            "Epoch 472/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3487 - accuracy: 0.8382 - precision: 0.8261 - auc: 0.9056 - recall: 0.8018 - val_loss: 0.3387 - val_accuracy: 0.8388 - val_precision: 0.8262 - val_auc: 0.9057 - val_recall: 0.8018\n",
            "Epoch 473/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3515 - accuracy: 0.8351 - precision: 0.8262 - auc: 0.9057 - recall: 0.8019 - val_loss: 0.3401 - val_accuracy: 0.8402 - val_precision: 0.8262 - val_auc: 0.9057 - val_recall: 0.8019\n",
            "Epoch 474/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3501 - accuracy: 0.8375 - precision: 0.8262 - auc: 0.9057 - recall: 0.8019 - val_loss: 0.3381 - val_accuracy: 0.8417 - val_precision: 0.8262 - val_auc: 0.9057 - val_recall: 0.8019\n",
            "Epoch 475/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3494 - accuracy: 0.8379 - precision: 0.8263 - auc: 0.9058 - recall: 0.8019 - val_loss: 0.3404 - val_accuracy: 0.8377 - val_precision: 0.8263 - val_auc: 0.9058 - val_recall: 0.8020\n",
            "Epoch 476/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3509 - accuracy: 0.8381 - precision: 0.8263 - auc: 0.9058 - recall: 0.8020 - val_loss: 0.3368 - val_accuracy: 0.8409 - val_precision: 0.8263 - val_auc: 0.9058 - val_recall: 0.8020\n",
            "Epoch 477/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3511 - accuracy: 0.8378 - precision: 0.8264 - auc: 0.9058 - recall: 0.8021 - val_loss: 0.3375 - val_accuracy: 0.8403 - val_precision: 0.8264 - val_auc: 0.9059 - val_recall: 0.8021\n",
            "Epoch 478/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3488 - accuracy: 0.8379 - precision: 0.8264 - auc: 0.9059 - recall: 0.8021 - val_loss: 0.3373 - val_accuracy: 0.8401 - val_precision: 0.8264 - val_auc: 0.9059 - val_recall: 0.8021\n",
            "Epoch 479/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3497 - accuracy: 0.8393 - precision: 0.8265 - auc: 0.9059 - recall: 0.8022 - val_loss: 0.3382 - val_accuracy: 0.8421 - val_precision: 0.8265 - val_auc: 0.9059 - val_recall: 0.8022\n",
            "Epoch 480/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3513 - accuracy: 0.8367 - precision: 0.8265 - auc: 0.9060 - recall: 0.8022 - val_loss: 0.3386 - val_accuracy: 0.8420 - val_precision: 0.8265 - val_auc: 0.9060 - val_recall: 0.8022\n",
            "Epoch 481/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3515 - accuracy: 0.8369 - precision: 0.8265 - auc: 0.9060 - recall: 0.8023 - val_loss: 0.3409 - val_accuracy: 0.8411 - val_precision: 0.8266 - val_auc: 0.9060 - val_recall: 0.8023\n",
            "Epoch 482/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3472 - accuracy: 0.8395 - precision: 0.8266 - auc: 0.9060 - recall: 0.8023 - val_loss: 0.3385 - val_accuracy: 0.8410 - val_precision: 0.8266 - val_auc: 0.9061 - val_recall: 0.8024\n",
            "Epoch 483/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3463 - accuracy: 0.8393 - precision: 0.8266 - auc: 0.9061 - recall: 0.8024 - val_loss: 0.3370 - val_accuracy: 0.8397 - val_precision: 0.8267 - val_auc: 0.9061 - val_recall: 0.8024\n",
            "Epoch 484/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3486 - accuracy: 0.8383 - precision: 0.8267 - auc: 0.9061 - recall: 0.8025 - val_loss: 0.3388 - val_accuracy: 0.8406 - val_precision: 0.8267 - val_auc: 0.9062 - val_recall: 0.8025\n",
            "Epoch 485/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3499 - accuracy: 0.8376 - precision: 0.8267 - auc: 0.9062 - recall: 0.8025 - val_loss: 0.3378 - val_accuracy: 0.8431 - val_precision: 0.8267 - val_auc: 0.9062 - val_recall: 0.8025\n",
            "Epoch 486/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3483 - accuracy: 0.8380 - precision: 0.8268 - auc: 0.9062 - recall: 0.8026 - val_loss: 0.3383 - val_accuracy: 0.8424 - val_precision: 0.8268 - val_auc: 0.9062 - val_recall: 0.8026\n",
            "Epoch 487/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3491 - accuracy: 0.8385 - precision: 0.8268 - auc: 0.9062 - recall: 0.8026 - val_loss: 0.3388 - val_accuracy: 0.8411 - val_precision: 0.8268 - val_auc: 0.9063 - val_recall: 0.8027\n",
            "Epoch 488/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3465 - accuracy: 0.8400 - precision: 0.8268 - auc: 0.9063 - recall: 0.8027 - val_loss: 0.3383 - val_accuracy: 0.8407 - val_precision: 0.8269 - val_auc: 0.9063 - val_recall: 0.8027\n",
            "Epoch 489/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3485 - accuracy: 0.8388 - precision: 0.8269 - auc: 0.9063 - recall: 0.8028 - val_loss: 0.3399 - val_accuracy: 0.8427 - val_precision: 0.8269 - val_auc: 0.9064 - val_recall: 0.8028\n",
            "Epoch 490/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3478 - accuracy: 0.8385 - precision: 0.8269 - auc: 0.9064 - recall: 0.8028 - val_loss: 0.3395 - val_accuracy: 0.8406 - val_precision: 0.8270 - val_auc: 0.9064 - val_recall: 0.8028\n",
            "Epoch 491/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3465 - accuracy: 0.8401 - precision: 0.8270 - auc: 0.9064 - recall: 0.8029 - val_loss: 0.3364 - val_accuracy: 0.8413 - val_precision: 0.8270 - val_auc: 0.9064 - val_recall: 0.8029\n",
            "Epoch 492/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3479 - accuracy: 0.8394 - precision: 0.8270 - auc: 0.9065 - recall: 0.8029 - val_loss: 0.3385 - val_accuracy: 0.8404 - val_precision: 0.8271 - val_auc: 0.9065 - val_recall: 0.8029\n",
            "Epoch 493/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3478 - accuracy: 0.8399 - precision: 0.8271 - auc: 0.9065 - recall: 0.8030 - val_loss: 0.3395 - val_accuracy: 0.8409 - val_precision: 0.8271 - val_auc: 0.9065 - val_recall: 0.8030\n",
            "Epoch 494/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3469 - accuracy: 0.8385 - precision: 0.8271 - auc: 0.9065 - recall: 0.8030 - val_loss: 0.3385 - val_accuracy: 0.8424 - val_precision: 0.8272 - val_auc: 0.9066 - val_recall: 0.8030\n",
            "Epoch 495/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3472 - accuracy: 0.8384 - precision: 0.8272 - auc: 0.9066 - recall: 0.8031 - val_loss: 0.3392 - val_accuracy: 0.8404 - val_precision: 0.8272 - val_auc: 0.9066 - val_recall: 0.8031\n",
            "Epoch 496/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3471 - accuracy: 0.8384 - precision: 0.8272 - auc: 0.9066 - recall: 0.8031 - val_loss: 0.3362 - val_accuracy: 0.8420 - val_precision: 0.8272 - val_auc: 0.9066 - val_recall: 0.8032\n",
            "Epoch 497/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3473 - accuracy: 0.8387 - precision: 0.8273 - auc: 0.9067 - recall: 0.8032 - val_loss: 0.3392 - val_accuracy: 0.8400 - val_precision: 0.8273 - val_auc: 0.9067 - val_recall: 0.8032\n",
            "Epoch 498/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3479 - accuracy: 0.8379 - precision: 0.8273 - auc: 0.9067 - recall: 0.8032 - val_loss: 0.3396 - val_accuracy: 0.8413 - val_precision: 0.8273 - val_auc: 0.9067 - val_recall: 0.8033\n",
            "Epoch 499/500\n",
            "37725/37725 [==============================] - 44s 1ms/step - loss: 0.3475 - accuracy: 0.8386 - precision: 0.8273 - auc: 0.9067 - recall: 0.8033 - val_loss: 0.3373 - val_accuracy: 0.8407 - val_precision: 0.8274 - val_auc: 0.9068 - val_recall: 0.8033\n",
            "Epoch 500/500\n",
            "37725/37725 [==============================] - 45s 1ms/step - loss: 0.3499 - accuracy: 0.8371 - precision: 0.8274 - auc: 0.9068 - recall: 0.8033 - val_loss: 0.3371 - val_accuracy: 0.8420 - val_precision: 0.8274 - val_auc: 0.9068 - val_recall: 0.8034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbuV2gNCs_6O"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RGNUviM2o0J"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o94yEZPTizyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5e5558-7a75-4af2-b9f1-f10cfa234d7c"
      },
      "source": [
        "#word count tokenizer (useful to define ANN parameters)\r\n",
        "x = sorted((tokenizer.word_index).items(), key=lambda x: x[1], reverse=True)\r\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('quadruplet', 19597),\n",
              " ('thaumaturgy', 19596),\n",
              " ('ink', 19595),\n",
              " ('emphasis', 19594),\n",
              " ('itchiness', 19593),\n",
              " ('noncitizen', 19592),\n",
              " ('protactinium', 19591),\n",
              " ('regal', 19590),\n",
              " ('lampblack', 19589),\n",
              " ('townsfolk', 19588),\n",
              " ('somersaulting', 19587),\n",
              " ('commons', 19586),\n",
              " ('rein', 19585),\n",
              " ('revered', 19584),\n",
              " ('noneffervescent', 19583),\n",
              " ('adequate', 19582),\n",
              " ('consultation', 19581),\n",
              " ('throwaway', 19580),\n",
              " ('spectre', 19579),\n",
              " ('swan', 19578),\n",
              " ('invariably', 19577),\n",
              " ('hum', 19576),\n",
              " ('parlor', 19575),\n",
              " ('inspire', 19574),\n",
              " ('yowl', 19573),\n",
              " ('fledgeling', 19572),\n",
              " ('diddle', 19571),\n",
              " ('elastic', 19570),\n",
              " ('moisture', 19569),\n",
              " ('whiteness', 19568),\n",
              " ('presenter', 19567),\n",
              " ('ironical', 19566),\n",
              " ('mugger', 19565),\n",
              " ('hugger', 19564),\n",
              " ('greenwich', 19563),\n",
              " ('motionless', 19562),\n",
              " ('budge', 19561),\n",
              " ('frizzy', 19560),\n",
              " ('strung', 19559),\n",
              " ('footnote', 19558),\n",
              " ('specie', 19557),\n",
              " ('mixer', 19556),\n",
              " ('portraiture', 19555),\n",
              " ('bollix', 19554),\n",
              " ('proofread', 19553),\n",
              " ('genu', 19552),\n",
              " ('septet', 19551),\n",
              " ('arrivederci', 19550),\n",
              " ('meliorate', 19549),\n",
              " ('acute', 19548),\n",
              " ('frightfully', 19547),\n",
              " ('curio', 19546),\n",
              " ('apprehension', 19545),\n",
              " ('vaporize', 19544),\n",
              " ('rhenium', 19543),\n",
              " ('eater', 19542),\n",
              " ('impute', 19541),\n",
              " ('diagram', 19540),\n",
              " ('marian', 19539),\n",
              " ('sulphurous', 19538),\n",
              " ('bookman', 19537),\n",
              " ('extolment', 19536),\n",
              " ('volaille', 19535),\n",
              " ('scripted', 19534),\n",
              " ('motivating', 19533),\n",
              " ('justly', 19532),\n",
              " ('howler', 19531),\n",
              " ('outflow', 19530),\n",
              " ('ample', 19529),\n",
              " ('plash', 19528),\n",
              " ('rivalry', 19527),\n",
              " ('mollify', 19526),\n",
              " ('guck', 19525),\n",
              " ('relish', 19524),\n",
              " ('resplendent', 19523),\n",
              " ('disadvantageously', 19522),\n",
              " ('automobile', 19521),\n",
              " ('drafting', 19520),\n",
              " ('hosni', 19519),\n",
              " ('unspoilt', 19518),\n",
              " ('volt', 19517),\n",
              " ('freehanded', 19516),\n",
              " ('hiss', 19515),\n",
              " ('envious', 19514),\n",
              " ('nonparallel', 19513),\n",
              " ('saltation', 19512),\n",
              " ('impairment', 19511),\n",
              " ('wolffish', 19510),\n",
              " ('cogent', 19509),\n",
              " ('rapt', 19508),\n",
              " ('encrypt', 19507),\n",
              " ('obstruct', 19506),\n",
              " ('subtitle', 19505),\n",
              " ('jove', 19504),\n",
              " ('multiplication', 19503),\n",
              " ('febrility', 19502),\n",
              " ('swordplay', 19501),\n",
              " ('mamas', 19500),\n",
              " ('interestingness', 19499),\n",
              " ('boastfully', 19498),\n",
              " ('plunge', 19497),\n",
              " ('anthropophagite', 19496),\n",
              " ('assess', 19495),\n",
              " ('regenerate', 19494),\n",
              " ('blaze', 19493),\n",
              " ('taut', 19492),\n",
              " ('trey', 19491),\n",
              " ('resettlement', 19490),\n",
              " ('tantalum', 19489),\n",
              " ('centennial', 19488),\n",
              " ('frank', 19487),\n",
              " ('orphic', 19486),\n",
              " ('overplus', 19485),\n",
              " ('astronomical', 19484),\n",
              " ('wink', 19483),\n",
              " ('litre', 19482),\n",
              " ('izzard', 19481),\n",
              " ('accessary', 19480),\n",
              " ('mulct', 19479),\n",
              " ('frigidness', 19478),\n",
              " ('fantasize', 19477),\n",
              " ('distaff', 19476),\n",
              " ('transverse', 19475),\n",
              " ('farewell', 19474),\n",
              " ('cognisant', 19473),\n",
              " ('dkg', 19472),\n",
              " ('tetri', 19471),\n",
              " ('camellia', 19470),\n",
              " ('alternating', 19469),\n",
              " ('moderately', 19468),\n",
              " ('continuation', 19467),\n",
              " ('informant', 19466),\n",
              " ('fiesta', 19465),\n",
              " ('divorcement', 19464),\n",
              " ('becharm', 19463),\n",
              " ('memoranda', 19462),\n",
              " ('retinue', 19461),\n",
              " ('patrol', 19460),\n",
              " ('enrapture', 19459),\n",
              " ('outgo', 19458),\n",
              " ('currency', 19457),\n",
              " ('tarradiddle', 19456),\n",
              " ('dysphoric', 19455),\n",
              " ('snog', 19454),\n",
              " ('countenance', 19453),\n",
              " ('permute', 19452),\n",
              " ('anchorite', 19451),\n",
              " ('ghastly', 19450),\n",
              " ('gourmandize', 19449),\n",
              " ('wakeless', 19448),\n",
              " ('engraving', 19447),\n",
              " ('bracing', 19446),\n",
              " ('telecasting', 19445),\n",
              " ('valiant', 19444),\n",
              " ('outcry', 19443),\n",
              " ('hunky', 19442),\n",
              " ('distich', 19441),\n",
              " ('logic', 19440),\n",
              " ('estimator', 19439),\n",
              " ('growl', 19438),\n",
              " ('hostel', 19437),\n",
              " ('impale', 19436),\n",
              " ('contribute', 19435),\n",
              " ('punishing', 19434),\n",
              " ('hereafter', 19433),\n",
              " ('marauder', 19432),\n",
              " ('partake', 19431),\n",
              " ('nullity', 19430),\n",
              " ('ignominious', 19429),\n",
              " ('posture', 19428),\n",
              " ('commix', 19427),\n",
              " ('spoonful', 19426),\n",
              " ('equilibrate', 19425),\n",
              " ('adjectival', 19424),\n",
              " ('severalise', 19423),\n",
              " ('collapses', 19422),\n",
              " ('sheer', 19421),\n",
              " ('existential', 19420),\n",
              " ('essentials', 19419),\n",
              " ('motorway', 19418),\n",
              " ('correlational', 19417),\n",
              " ('evenfall', 19416),\n",
              " ('guardianship', 19415),\n",
              " ('sapience', 19414),\n",
              " ('gemstone', 19413),\n",
              " ('francis', 19412),\n",
              " ('rocknroll', 19411),\n",
              " ('pillowcase', 19410),\n",
              " ('victimization', 19409),\n",
              " ('native', 19408),\n",
              " ('arrant', 19407),\n",
              " ('lofty', 19406),\n",
              " ('meditate', 19405),\n",
              " ('pecker', 19404),\n",
              " ('surreptitious', 19403),\n",
              " ('headliner', 19402),\n",
              " ('cultivation', 19401),\n",
              " ('barefaced', 19400),\n",
              " ('glucocorticoid', 19399),\n",
              " ('shoring', 19398),\n",
              " ('kib', 19397),\n",
              " ('goad', 19396),\n",
              " ('bowlful', 19395),\n",
              " ('yangtze', 19394),\n",
              " ('unbeatable', 19393),\n",
              " ('pigmy', 19392),\n",
              " ('albert', 19391),\n",
              " ('edgar', 19390),\n",
              " ('seldom', 19389),\n",
              " ('francophil', 19388),\n",
              " ('omit', 19387),\n",
              " ('cunt', 19386),\n",
              " ('peel', 19385),\n",
              " ('fingernail', 19384),\n",
              " ('entrant', 19383),\n",
              " ('compassionateness', 19382),\n",
              " ('gimpy', 19381),\n",
              " ('bushel', 19380),\n",
              " ('irregular', 19379),\n",
              " ('michigander', 19378),\n",
              " ('paraphernalia', 19377),\n",
              " ('mugs', 19376),\n",
              " ('magnets', 19375),\n",
              " ('feminine', 19374),\n",
              " ('macrocosm', 19373),\n",
              " ('sumptuosity', 19372),\n",
              " ('finality', 19371),\n",
              " ('superfluity', 19370),\n",
              " ('citizens', 19369),\n",
              " ('hairdresser', 19368),\n",
              " ('enwrap', 19367),\n",
              " ('evelina', 19366),\n",
              " ('julia', 19365),\n",
              " ('imperial', 19364),\n",
              " ('interlingual', 19363),\n",
              " ('professing', 19362),\n",
              " ('tartar', 19361),\n",
              " ('fingerbreadth', 19360),\n",
              " ('nutcase', 19359),\n",
              " ('borecole', 19358),\n",
              " ('parkinson', 19357),\n",
              " ('northcote', 19356),\n",
              " ('cyril', 19355),\n",
              " ('decker', 19354),\n",
              " ('simulator', 19353),\n",
              " ('acquaintance', 19352),\n",
              " ('glossiness', 19351),\n",
              " ('initiative', 19350),\n",
              " ('prey', 19349),\n",
              " ('marxist', 19348),\n",
              " ('scant', 19347),\n",
              " ('knead', 19346),\n",
              " ('sole', 19345),\n",
              " ('magnetize', 19344),\n",
              " ('hemipteran', 19343),\n",
              " ('stinky', 19342),\n",
              " ('perfidiousness', 19341),\n",
              " ('creeping', 19340),\n",
              " ('damp', 19339),\n",
              " ('nimbus', 19338),\n",
              " ('hitch', 19337),\n",
              " ('analytic', 19336),\n",
              " ('who', 19335),\n",
              " ('farrow', 19334),\n",
              " ('appease', 19333),\n",
              " ('maniac', 19332),\n",
              " ('ineffectual', 19331),\n",
              " ('twosome', 19330),\n",
              " ('dispatch', 19329),\n",
              " ('spitting', 19328),\n",
              " ('chirk', 19327),\n",
              " ('advertizing', 19326),\n",
              " ('fiend', 19325),\n",
              " ('fume', 19324),\n",
              " ('nightclub', 19323),\n",
              " ('longshoreman', 19322),\n",
              " ('insemination', 19321),\n",
              " ('artificial', 19320),\n",
              " ('shrub', 19319),\n",
              " ('shark', 19318),\n",
              " ('catacomb', 19317),\n",
              " ('intercept', 19316),\n",
              " ('unending', 19315),\n",
              " ('she', 19314),\n",
              " ('stooge', 19313),\n",
              " ('unify', 19312),\n",
              " ('conflict', 19311),\n",
              " ('indirect', 19310),\n",
              " ('deutschland', 19309),\n",
              " ('sob', 19308),\n",
              " ('caw', 19307),\n",
              " ('cagy', 19306),\n",
              " ('aluminium', 19305),\n",
              " ('flatboat', 19304),\n",
              " ('gormandise', 19303),\n",
              " ('fable', 19302),\n",
              " ('atone', 19301),\n",
              " ('jewel', 19300),\n",
              " ('idealist', 19299),\n",
              " ('husky', 19298),\n",
              " ('ungratified', 19297),\n",
              " ('rhody', 19296),\n",
              " ('ownpropname', 19295),\n",
              " ('morphological', 19294),\n",
              " ('neb', 19293),\n",
              " ('ardor', 19292),\n",
              " ('dilute', 19291),\n",
              " ('terrific', 19290),\n",
              " ('adage', 19289),\n",
              " ('until', 19288),\n",
              " ('colloidal', 19287),\n",
              " ('impose', 19286),\n",
              " ('climax', 19285),\n",
              " ('creek', 19284),\n",
              " ('jinx', 19283),\n",
              " ('witting', 19282),\n",
              " ('goofball', 19281),\n",
              " ('aggroup', 19280),\n",
              " ('terminated', 19279),\n",
              " ('sativus', 19278),\n",
              " ('cucumis', 19277),\n",
              " ('shoetree', 19276),\n",
              " ('impropriety', 19275),\n",
              " ('cablegram', 19274),\n",
              " ('virtually', 19273),\n",
              " ('fidgety', 19272),\n",
              " ('barleycorn', 19271),\n",
              " ('newsprint', 19270),\n",
              " ('tomentum', 19269),\n",
              " ('dampish', 19268),\n",
              " ('profess', 19267),\n",
              " ('appoint', 19266),\n",
              " ('crepuscule', 19265),\n",
              " ('irak', 19264),\n",
              " ('thinned', 19263),\n",
              " ('chriastmas', 19262),\n",
              " ('grillroom', 19261),\n",
              " ('traumatize', 19260),\n",
              " ('fighter', 19259),\n",
              " ('nor', 19258),\n",
              " ('dejection', 19257),\n",
              " ('penalisation', 19256),\n",
              " ('honcho', 19255),\n",
              " ('diaphragm', 19254),\n",
              " ('pocketbook', 19253),\n",
              " ('molest', 19252),\n",
              " ('rarefied', 19251),\n",
              " ('fogy', 19250),\n",
              " ('photographic', 19249),\n",
              " ('nationalist', 19248),\n",
              " ('bodoni', 19247),\n",
              " ('anatole', 19246),\n",
              " ('adorn', 19245),\n",
              " ('ashen', 19244),\n",
              " ('rime', 19243),\n",
              " ('consort', 19242),\n",
              " ('climate', 19241),\n",
              " ('recite', 19240),\n",
              " ('orb', 19239),\n",
              " ('imply', 19238),\n",
              " ('leaden', 19237),\n",
              " ('dauntless', 19236),\n",
              " ('quint', 19235),\n",
              " ('qat', 19234),\n",
              " ('notional', 19233),\n",
              " ('designation', 19232),\n",
              " ('winfield', 19231),\n",
              " ('despicable', 19230),\n",
              " ('leicester', 19229),\n",
              " ('watercraft', 19228),\n",
              " ('disposed', 19227),\n",
              " ('plenteous', 19226),\n",
              " ('terminalis', 19225),\n",
              " ('cordyline', 19224),\n",
              " ('mensurate', 19223),\n",
              " ('actinium', 19222),\n",
              " ('kthxbi', 19221),\n",
              " ('regurgitate', 19220),\n",
              " ('antoine', 19219),\n",
              " ('jacques', 19218),\n",
              " ('flog', 19217),\n",
              " ('involvement', 19216),\n",
              " ('veterinarian', 19215),\n",
              " ('aimed', 19214),\n",
              " ('gip', 19213),\n",
              " ('naut', 19212),\n",
              " ('afield', 19211),\n",
              " ('indigence', 19210),\n",
              " ('clew', 19209),\n",
              " ('tridactylus', 19208),\n",
              " ('bradypus', 19207),\n",
              " ('sprain', 19206),\n",
              " ('most', 19205),\n",
              " ('coating', 19204),\n",
              " ('athirst', 19203),\n",
              " ('menial', 19202),\n",
              " ('reverberation', 19201),\n",
              " ('zest', 19200),\n",
              " ('ingurgitate', 19199),\n",
              " ('aweless', 19198),\n",
              " ('unobjectionable', 19197),\n",
              " ('objection', 19196),\n",
              " ('festinate', 19195),\n",
              " ('moisten', 19194),\n",
              " ('menstruation', 19193),\n",
              " ('ameliorate', 19192),\n",
              " ('aire', 19191),\n",
              " ('daimon', 19190),\n",
              " ('aflame', 19189),\n",
              " ('malice', 19188),\n",
              " ('cubicle', 19187),\n",
              " ('islamiyah', 19186),\n",
              " ('jemaah', 19185),\n",
              " ('isobilateral', 19184),\n",
              " ('whip', 19183),\n",
              " ('humankind', 19182),\n",
              " ('exhaust', 19181),\n",
              " ('wellspring', 19180),\n",
              " ('lull', 19179),\n",
              " ('scots', 19178),\n",
              " ('sprayer', 19177),\n",
              " ('nonvoluntary', 19176),\n",
              " ('butcher', 19175),\n",
              " ('stimulation', 19174),\n",
              " ('advertize', 19173),\n",
              " ('openhanded', 19172),\n",
              " ('conception', 19171),\n",
              " ('remove', 19170),\n",
              " ('incarnation', 19169),\n",
              " ('administrator', 19168),\n",
              " ('announcer', 19167),\n",
              " ('broody', 19166),\n",
              " ('pieplant', 19165),\n",
              " ('nisan', 19164),\n",
              " ('squab', 19163),\n",
              " ('cabinet', 19162),\n",
              " ('from', 19161),\n",
              " ('cerebrovascular', 19160),\n",
              " ('sufferance', 19159),\n",
              " ('netmail', 19158),\n",
              " ('ternion', 19157),\n",
              " ('sanchez', 19156),\n",
              " ('ilich', 19155),\n",
              " ('commodious', 19154),\n",
              " ('wrench', 19153),\n",
              " ('mediaeval', 19152),\n",
              " ('diversion', 19151),\n",
              " ('dense', 19150),\n",
              " ('nooky', 19149),\n",
              " ('purposeless', 19148),\n",
              " ('ruefulness', 19147),\n",
              " ('manor', 19146),\n",
              " ('copal', 19145),\n",
              " ('zanzibar', 19144),\n",
              " ('thrash', 19143),\n",
              " ('caveman', 19142),\n",
              " ('geographical', 19141),\n",
              " ('featherbed', 19140),\n",
              " ('lexicon', 19139),\n",
              " ('sidekick', 19138),\n",
              " ('intimation', 19137),\n",
              " ('cupboard', 19136),\n",
              " ('countersink', 19135),\n",
              " ('wyrd', 19134),\n",
              " ('plainly', 19133),\n",
              " ('wrongdoer', 19132),\n",
              " ('shinny', 19131),\n",
              " ('sweetness', 19130),\n",
              " ('ampere', 19129),\n",
              " ('liken', 19128),\n",
              " ('advent', 19127),\n",
              " ('muckle', 19126),\n",
              " ('laurus', 19125),\n",
              " ('adjudge', 19124),\n",
              " ('garrison', 19123),\n",
              " ('spate', 19122),\n",
              " ('liable', 19121),\n",
              " ('hayfield', 19120),\n",
              " ('fascia', 19119),\n",
              " ('hellenic', 19118),\n",
              " ('muddle', 19117),\n",
              " ('clumsy', 19116),\n",
              " ('diminish', 19115),\n",
              " ('exculpation', 19114),\n",
              " ('larder', 19113),\n",
              " ('attain', 19112),\n",
              " ('rink', 19111),\n",
              " ('rumination', 19110),\n",
              " ('julio', 19109),\n",
              " ('andrew', 19108),\n",
              " ('agglomerate', 19107),\n",
              " ('tempt', 19106),\n",
              " ('razzmatazz', 19105),\n",
              " ('tercet', 19104),\n",
              " ('counsel', 19103),\n",
              " ('deprave', 19102),\n",
              " ('accommodation', 19101),\n",
              " ('qualifier', 19100),\n",
              " ('aqualung', 19099),\n",
              " ('harmonize', 19098),\n",
              " ('he', 19097),\n",
              " ('unbarred', 19096),\n",
              " ('toad', 19095),\n",
              " ('pits', 19094),\n",
              " ('sewerage', 19093),\n",
              " ('puke', 19092),\n",
              " ('genuflect', 19091),\n",
              " ('sequence', 19090),\n",
              " ('whop', 19089),\n",
              " ('postscript', 19088),\n",
              " ('elemental', 19087),\n",
              " ('immature', 19086),\n",
              " ('hub', 19085),\n",
              " ('slime', 19084),\n",
              " ('token', 19083),\n",
              " ('spanking', 19082),\n",
              " ('puritanic', 19081),\n",
              " ('tissue', 19080),\n",
              " ('osseous', 19079),\n",
              " ('dazed', 19078),\n",
              " ('commercialize', 19077),\n",
              " ('cristal', 19076),\n",
              " ('plancks', 19075),\n",
              " ('unwell', 19074),\n",
              " ('existing', 19073),\n",
              " ('barter', 19072),\n",
              " ('advertisement', 19071),\n",
              " ('mansion', 19070),\n",
              " ('fastball', 19069),\n",
              " ('rudolf', 19068),\n",
              " ('corking', 19067),\n",
              " ('hoar', 19066),\n",
              " ('severance', 19065),\n",
              " ('acclivitous', 19064),\n",
              " ('ioway', 19063),\n",
              " ('grin', 19062),\n",
              " ('gunshot', 19061),\n",
              " ('nightfall', 19060),\n",
              " ('graves', 19059),\n",
              " ('elisha', 19058),\n",
              " ('reckoner', 19057),\n",
              " ('sovereign', 19056),\n",
              " ('soused', 19055),\n",
              " ('trolling', 19054),\n",
              " ('fawn', 19053),\n",
              " ('mourn', 19052),\n",
              " ('reincarnate', 19051),\n",
              " ('sweeping', 19050),\n",
              " ('helianthus', 19049),\n",
              " ('magpie', 19048),\n",
              " ('enthral', 19047),\n",
              " ('imposture', 19046),\n",
              " ('middling', 19045),\n",
              " ('petabit', 19044),\n",
              " ('clot', 19043),\n",
              " ('qualification', 19042),\n",
              " ('aplomb', 19041),\n",
              " ('trench', 19040),\n",
              " ('bulb', 19039),\n",
              " ('tarawa', 19038),\n",
              " ('reprieve', 19037),\n",
              " ('embracing', 19036),\n",
              " ('sinfulness', 19035),\n",
              " ('rainfly', 19034),\n",
              " ('procession', 19033),\n",
              " ('astronomic', 19032),\n",
              " ('frontward', 19031),\n",
              " ('villainous', 19030),\n",
              " ('priming', 19029),\n",
              " ('funniness', 19028),\n",
              " ('foramen', 19027),\n",
              " ('forethought', 19026),\n",
              " ('buckyball', 19025),\n",
              " ('inebriate', 19024),\n",
              " ('operating', 19023),\n",
              " ('graven', 19022),\n",
              " ('panel', 19021),\n",
              " ('hock', 19020),\n",
              " ('tug', 19019),\n",
              " ('gladiola', 19018),\n",
              " ('jewellery', 19017),\n",
              " ('bard', 19016),\n",
              " ('benni', 19015),\n",
              " ('shag', 19014),\n",
              " ('bouldered', 19013),\n",
              " ('lingo', 19012),\n",
              " ('paralyse', 19011),\n",
              " ('ravisher', 19010),\n",
              " ('overbold', 19009),\n",
              " ('pouf', 19008),\n",
              " ('decrypt', 19007),\n",
              " ('miscarriage', 19006),\n",
              " ('curler', 19005),\n",
              " ('dweller', 19004),\n",
              " ('grayish', 19003),\n",
              " ('instill', 19002),\n",
              " ('spicery', 19001),\n",
              " ('bloodline', 19000),\n",
              " ('cowhand', 18999),\n",
              " ('kyd', 18998),\n",
              " ('aforesaid', 18997),\n",
              " ('renown', 18996),\n",
              " ('knight', 18995),\n",
              " ('solitaria', 18994),\n",
              " ('pezophaps', 18993),\n",
              " ('incidental', 18992),\n",
              " ('possess', 18991),\n",
              " ('cc', 18990),\n",
              " ('risible', 18989),\n",
              " ('debar', 18988),\n",
              " ('enunciate', 18987),\n",
              " ('putt', 18986),\n",
              " ('charr', 18985),\n",
              " ('ronald', 18984),\n",
              " ('fiddling', 18983),\n",
              " ('rod', 18982),\n",
              " ('inconspicuous', 18981),\n",
              " ('samoa', 18980),\n",
              " ('ember', 18979),\n",
              " ('repose', 18978),\n",
              " ('emollient', 18977),\n",
              " ('quem', 18976),\n",
              " ('dialect', 18975),\n",
              " ('supervisory', 18974),\n",
              " ('kangaroo', 18973),\n",
              " ('tenting', 18972),\n",
              " ('grape', 18971),\n",
              " ('extremity', 18970),\n",
              " ('matte', 18969),\n",
              " ('xizang', 18968),\n",
              " ('tidy', 18967),\n",
              " ('establish', 18966),\n",
              " ('descend', 18965),\n",
              " ('playact', 18964),\n",
              " ('glistening', 18963),\n",
              " ('speculation', 18962),\n",
              " ('salubriousness', 18961),\n",
              " ('schizophrenia', 18960),\n",
              " ('catatonic', 18959),\n",
              " ('neglige', 18958),\n",
              " ('optimisation', 18957),\n",
              " ('rend', 18956),\n",
              " ('calibrate', 18955),\n",
              " ('riff', 18954),\n",
              " ('hullo', 18953),\n",
              " ('chilling', 18952),\n",
              " ('sustain', 18951),\n",
              " ('harass', 18950),\n",
              " ('kayak', 18949),\n",
              " ('swinging', 18948),\n",
              " ('ancestry', 18947),\n",
              " ('regime', 18946),\n",
              " ('scum', 18945),\n",
              " ('hobbyhorse', 18944),\n",
              " ('obturate', 18943),\n",
              " ('populate', 18942),\n",
              " ('importune', 18941),\n",
              " ('aquiline', 18940),\n",
              " ('bowler', 18939),\n",
              " ('fidel', 18938),\n",
              " ('fictile', 18937),\n",
              " ('romany', 18936),\n",
              " ('nauseous', 18935),\n",
              " ('dire', 18934),\n",
              " ('outright', 18933),\n",
              " ('lynx', 18932),\n",
              " ('metier', 18931),\n",
              " ('diddley', 18930),\n",
              " ('smuggled', 18929),\n",
              " ('revoltingly', 18928),\n",
              " ('unseasoned', 18927),\n",
              " ('jab', 18926),\n",
              " ('thorn', 18925),\n",
              " ('apprehensively', 18924),\n",
              " ('azide', 18923),\n",
              " ('contrariness', 18922),\n",
              " ('osculate', 18921),\n",
              " ('octoberfest', 18920),\n",
              " ('risky', 18919),\n",
              " ('culture', 18918),\n",
              " ('humor', 18917),\n",
              " ('recollective', 18916),\n",
              " ('lotion', 18915),\n",
              " ('famed', 18914),\n",
              " ('nipper', 18913),\n",
              " ('eec', 18912),\n",
              " ('suspension', 18911),\n",
              " ('taipeh', 18910),\n",
              " ('buff', 18909),\n",
              " ('acephala', 18908),\n",
              " ('oleracea', 18907),\n",
              " ('brassica', 18906),\n",
              " ('protestation', 18905),\n",
              " ('diddly', 18904),\n",
              " ('seance', 18903),\n",
              " ('kilocycle', 18902),\n",
              " ('veil', 18901),\n",
              " ('traumatise', 18900),\n",
              " ('chopfallen', 18899),\n",
              " ('sternly', 18898),\n",
              " ('seize', 18897),\n",
              " ('phlebotomize', 18896),\n",
              " ('feud', 18895),\n",
              " ('bolshy', 18894),\n",
              " ('muted', 18893),\n",
              " ('arc', 18892),\n",
              " ('habitus', 18891),\n",
              " ('naval', 18890),\n",
              " ('incapacitate', 18889),\n",
              " ('neighbour', 18888),\n",
              " ('invalidate', 18887),\n",
              " ('greyness', 18886),\n",
              " ('regorge', 18885),\n",
              " ('maneuver', 18884),\n",
              " ('chitchat', 18883),\n",
              " ('duplicate', 18882),\n",
              " ('ostentate', 18881),\n",
              " ('dickhead', 18880),\n",
              " ('repast', 18879),\n",
              " ('scooter', 18878),\n",
              " ('whiz', 18877),\n",
              " ('outdo', 18876),\n",
              " ('occupied', 18875),\n",
              " ('insensate', 18874),\n",
              " ('rive', 18873),\n",
              " ('impalpable', 18872),\n",
              " ('foregone', 18871),\n",
              " ('muteness', 18870),\n",
              " ('slenderly', 18869),\n",
              " ('predominate', 18868),\n",
              " ('chaff', 18867),\n",
              " ('horrify', 18866),\n",
              " ('smarmy', 18865),\n",
              " ('rhapsodic', 18864),\n",
              " ('plum', 18863),\n",
              " ('betting', 18862),\n",
              " ('booster', 18861),\n",
              " ('vernacular', 18860),\n",
              " ('crummy', 18859),\n",
              " ('gallic', 18858),\n",
              " ('someways', 18857),\n",
              " ('albidum', 18856),\n",
              " ('medico', 18855),\n",
              " ('softheaded', 18854),\n",
              " ('prophesier', 18853),\n",
              " ('furbish', 18852),\n",
              " ('bop', 18851),\n",
              " ('delectable', 18850),\n",
              " ('anguish', 18849),\n",
              " ('appreciation', 18848),\n",
              " ('nutritive', 18847),\n",
              " ('unloosen', 18846),\n",
              " ('elwyn', 18845),\n",
              " ('hades', 18844),\n",
              " ('erroneousness', 18843),\n",
              " ('brobdingnagian', 18842),\n",
              " ('pullulate', 18841),\n",
              " ('thievery', 18840),\n",
              " ('rile', 18839),\n",
              " ('billet', 18838),\n",
              " ('orangish', 18837),\n",
              " ('calamary', 18836),\n",
              " ('omega', 18835),\n",
              " ('facial', 18834),\n",
              " ('incidentally', 18833),\n",
              " ('notation', 18832),\n",
              " ('purgation', 18831),\n",
              " ('wallpaper', 18830),\n",
              " ('busbar', 18829),\n",
              " ('compulsion', 18828),\n",
              " ('hirsute', 18827),\n",
              " ('judder', 18826),\n",
              " ('contraceptive', 18825),\n",
              " ('debilitation', 18824),\n",
              " ('fleck', 18823),\n",
              " ('romani', 18822),\n",
              " ('gadfly', 18821),\n",
              " ('pest', 18820),\n",
              " ('flow', 18819),\n",
              " ('septenary', 18818),\n",
              " ('chopper', 18817),\n",
              " ('uproarious', 18816),\n",
              " ('enterprise', 18815),\n",
              " ('bask', 18814),\n",
              " ('occlusion', 18813),\n",
              " ('devotee', 18812),\n",
              " ('reheel', 18811),\n",
              " ('forthcoming', 18810),\n",
              " ('playpen', 18809),\n",
              " ('metrical', 18808),\n",
              " ('will', 18807),\n",
              " ('argot', 18806),\n",
              " ('befuddled', 18805),\n",
              " ('beatified', 18804),\n",
              " ('honorable', 18803),\n",
              " ('nath', 18802),\n",
              " ('satyendra', 18801),\n",
              " ('weighed', 18800),\n",
              " ('arithmetic', 18799),\n",
              " ('marvellous', 18798),\n",
              " ('lactate', 18797),\n",
              " ('cambrian', 18796),\n",
              " ('hoop', 18795),\n",
              " ('assassinator', 18794),\n",
              " ('battercake', 18793),\n",
              " ('revue', 18792),\n",
              " ('uncertain', 18791),\n",
              " ('nowrecovery', 18790),\n",
              " ('locomotive', 18789),\n",
              " ('affright', 18788),\n",
              " ('demolish', 18787),\n",
              " ('overtake', 18786),\n",
              " ('tack', 18785),\n",
              " ('exterior', 18784),\n",
              " ('honours', 18783),\n",
              " ('tire', 18782),\n",
              " ('brainchild', 18781),\n",
              " ('precedence', 18780),\n",
              " ('periodic', 18779),\n",
              " ('ruffle', 18778),\n",
              " ('headman', 18777),\n",
              " ('unfold', 18776),\n",
              " ('sleazy', 18775),\n",
              " ('actuate', 18774),\n",
              " ('familiaris', 18773),\n",
              " ('canis', 18772),\n",
              " ('fleece', 18771),\n",
              " ('tickle', 18770),\n",
              " ('scribble', 18769),\n",
              " ('hollow', 18768),\n",
              " ('eisenhower', 18767),\n",
              " ('finalise', 18766),\n",
              " ('blueing', 18765),\n",
              " ('complicate', 18764),\n",
              " ('colt', 18763),\n",
              " ('border', 18762),\n",
              " ('legitimatize', 18761),\n",
              " ('oflahertie', 18760),\n",
              " ('fingal', 18759),\n",
              " ('concealing', 18758),\n",
              " ('befit', 18757),\n",
              " ('disgustful', 18756),\n",
              " ('aerate', 18755),\n",
              " ('inter', 18754),\n",
              " ('fustian', 18753),\n",
              " ('swapped', 18752),\n",
              " ('dribble', 18751),\n",
              " ('serenade', 18750),\n",
              " ('potency', 18749),\n",
              " ('scenery', 18748),\n",
              " ('boon', 18747),\n",
              " ('curbing', 18746),\n",
              " ('consummate', 18745),\n",
              " ('sorcerous', 18744),\n",
              " ('knitwork', 18743),\n",
              " ('blockheaded', 18742),\n",
              " ('snapping', 18741),\n",
              " ('fashioned', 18740),\n",
              " ('blur', 18739),\n",
              " ('unification', 18738),\n",
              " ('minister', 18737),\n",
              " ('recap', 18736),\n",
              " ('combining', 18735),\n",
              " ('veneration', 18734),\n",
              " ('apprise', 18733),\n",
              " ('governance', 18732),\n",
              " ('sap', 18731),\n",
              " ('nookie', 18730),\n",
              " ('saucer', 18729),\n",
              " ('entreaty', 18728),\n",
              " ('blab', 18727),\n",
              " ('midway', 18726),\n",
              " ('fantasise', 18725),\n",
              " ('cymric', 18724),\n",
              " ('swale', 18723),\n",
              " ('viewer', 18722),\n",
              " ('tolerant', 18721),\n",
              " ('powdered', 18720),\n",
              " ('vagrant', 18719),\n",
              " ('gipsy', 18718),\n",
              " ('vociferation', 18717),\n",
              " ('infotainment', 18716),\n",
              " ('tatty', 18715),\n",
              " ('fabricate', 18714),\n",
              " ('officious', 18713),\n",
              " ('flagpole', 18712),\n",
              " ('breaker', 18711),\n",
              " ('roost', 18710),\n",
              " ('temptress', 18709),\n",
              " ('ropy', 18708),\n",
              " ('goodby', 18707),\n",
              " ('armoured', 18706),\n",
              " ('xtc', 18705),\n",
              " ('thrifty', 18704),\n",
              " ('bootleg', 18703),\n",
              " ('mensurable', 18702),\n",
              " ('coupon', 18701),\n",
              " ('indorse', 18700),\n",
              " ('millimetre', 18699),\n",
              " ('gastrointestinal', 18698),\n",
              " ('secondment', 18697),\n",
              " ('donna', 18696),\n",
              " ('prima', 18695),\n",
              " ('afters', 18694),\n",
              " ('judaic', 18693),\n",
              " ('airless', 18692),\n",
              " ('shangri', 18691),\n",
              " ('lakes', 18690),\n",
              " ('hush', 18689),\n",
              " ('sprinkle', 18688),\n",
              " ('curacoa', 18687),\n",
              " ('wrothful', 18686),\n",
              " ('subtraction', 18685),\n",
              " ('prop', 18684),\n",
              " ('consequencefree', 18683),\n",
              " ('peck', 18682),\n",
              " ('yearling', 18681),\n",
              " ('projectile', 18680),\n",
              " ('farad', 18679),\n",
              " ('foretell', 18678),\n",
              " ('meanspirited', 18677),\n",
              " ('steamroller', 18676),\n",
              " ('embarkation', 18675),\n",
              " ('piffling', 18674),\n",
              " ('illiberal', 18673),\n",
              " ('alphabetic', 18672),\n",
              " ('fold', 18671),\n",
              " ('pixy', 18670),\n",
              " ('imbecile', 18669),\n",
              " ('saneness', 18668),\n",
              " ('tamp', 18667),\n",
              " ('slack', 18666),\n",
              " ('exempt', 18665),\n",
              " ('paries', 18664),\n",
              " ('brainsick', 18663),\n",
              " ('originate', 18662),\n",
              " ('taliaferro', 18661),\n",
              " ('booker', 18660),\n",
              " ('parkland', 18659),\n",
              " ('aggrieve', 18658),\n",
              " ('smutty', 18657),\n",
              " ('fortify', 18656),\n",
              " ('quintuplet', 18655),\n",
              " ('bluster', 18654),\n",
              " ('copiousness', 18653),\n",
              " ('scrape', 18652),\n",
              " ('duct', 18651),\n",
              " ('epithelial', 18650),\n",
              " ('micturate', 18649),\n",
              " ('tiresome', 18648),\n",
              " ('escapist', 18647),\n",
              " ('endorsement', 18646),\n",
              " ('djinny', 18645),\n",
              " ('wondrous', 18644),\n",
              " ('unsatisfied', 18643),\n",
              " ('wrecking', 18642),\n",
              " ('carnival', 18641),\n",
              " ('enlistment', 18640),\n",
              " ('zeylanicum', 18639),\n",
              " ('cinnamomum', 18638),\n",
              " ('smooch', 18637),\n",
              " ('gayly', 18636),\n",
              " ('wheal', 18635),\n",
              " ('snowman', 18634),\n",
              " ('abominable', 18633),\n",
              " ('unsportsmanlike', 18632),\n",
              " ('seism', 18631),\n",
              " ('titter', 18630),\n",
              " ('logarithm', 18629),\n",
              " ('genitourinary', 18628),\n",
              " ('deficit', 18627),\n",
              " ('cognizance', 18626),\n",
              " ('deservingness', 18625),\n",
              " ('palms', 18624),\n",
              " ('ownership', 18623),\n",
              " ('choreography', 18622),\n",
              " ('recoil', 18621),\n",
              " ('nidus', 18620),\n",
              " ('wiseness', 18619),\n",
              " ('backspacer', 18618),\n",
              " ('indoctrinate', 18617),\n",
              " ('trapper', 18616),\n",
              " ('infantry', 18615),\n",
              " ('farmer', 18614),\n",
              " ('probable', 18613),\n",
              " ('datum', 18612),\n",
              " ('astonied', 18611),\n",
              " ('emphatically', 18610),\n",
              " ('likable', 18609),\n",
              " ('unwavering', 18608),\n",
              " ('stony', 18607),\n",
              " ('sib', 18606),\n",
              " ('plucky', 18605),\n",
              " ('engender', 18604),\n",
              " ('intestine', 18603),\n",
              " ('conglomerate', 18602),\n",
              " ('transplant', 18601),\n",
              " ('burrow', 18600),\n",
              " ('bacterium', 18599),\n",
              " ('nerveless', 18598),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap89kR0mbSzF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "20e7fe54-9df2-4296-de96-425636a20b76"
      },
      "source": [
        "#plot\r\n",
        "counts = []\r\n",
        "df_status = df.drop(['STATUS'], axis=1)\r\n",
        "categories = list(df_status.columns.values)\r\n",
        "for i in categories:\r\n",
        "    counts.append((i, df_status[i].sum()))\r\n",
        "df_stats = pd.DataFrame(counts, columns=['category', 'number_of_comments'])\r\n",
        "\r\n",
        "df_stats.plot(x='category', y='number_of_comments', kind='bar', legend=False, grid=True, figsize=(8, 5))\r\n",
        "plt.title(\"Nmero de atualizaes de status em cada fator\")\r\n",
        "plt.ylabel('# ocorrncias', fontsize=12)\r\n",
        "plt.xlabel('fator', fontsize=12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-56b2b61e4471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'number_of_comments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'number_of_comments'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nmero de atualizaes de status em cada fator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'# ocorrncias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_matplotlib/__init__.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(data, kind, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ax\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left_ax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLOT_CLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# no non-numeric frames or series allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no numeric data to plot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# GH25587: cast ExtensionArray of pandas (IntegerArray, etc.) to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: no numeric data to plot"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n2w43dtiQti",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "d1f6f86c-977c-4724-b8cb-4b7a19ffbab9"
      },
      "source": [
        "#plot\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "rowsums = df.iloc[:,2:].sum(axis=1)\r\n",
        "x=rowsums.value_counts()\r\n",
        "#plot\r\n",
        "plt.figure(figsize=(8,5))\r\n",
        "ax = sns.barplot(x.index, x.values)\r\n",
        "plt.title(\"Nmero de atualizaes de status que possuem multiplos fatores\")\r\n",
        "plt.ylabel('# de ocorrncias', fontsize=12)\r\n",
        "plt.xlabel('# de fatores', fontsize=12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, '# de fatores')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqz0SwmFisOu"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\r\n",
        "\r\n",
        "#model plot\r\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgjOYSALiu7L"
      },
      "source": [
        "#plot\r\n",
        "totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\r\n",
        "plt.hist(totalNumWords,bins = np.arange(0,410,10))\r\n",
        "plt.title(\"Nmero de palavras por sentena\")\r\n",
        "plt.ylabel('# de sentenas', fontsize=12)\r\n",
        "plt.xlabel('# de palavras', fontsize=12)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geOzy-1bio0E"
      },
      "source": [
        "#wordcloud\r\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "stopwords = set(STOPWORDS)\r\n",
        "stopwords.update([\"propname\"])\r\n",
        "\r\n",
        "text = \" \".join(review for review in df['STATUS'])\r\n",
        "\r\n",
        "wordcloud = WordCloud(background_color=\"black\",\r\n",
        "                      width=1600, height=800).generate(text)\r\n",
        "\r\n",
        "# Display the generated image:\r\n",
        "# the matplotlib way:\r\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\r\n",
        "plt.axis(\"off\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "wordcloud.to_file(\"wordcloud.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt8cdjvnooBr"
      },
      "source": [
        "#metric plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#auc\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "\n",
        "plt.title('model auc')\n",
        "plt.ylabel('auc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#precision\n",
        "plt.plot(history.history['precision'])\n",
        "plt.plot(history.history['val_precision'])\n",
        "\n",
        "plt.title('model precision')\n",
        "plt.ylabel('precision')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#recall\n",
        "plt.plot(history.history['recall'])\n",
        "plt.plot(history.history['val_recall'])\n",
        "\n",
        "plt.title('model recall')\n",
        "plt.ylabel('recall')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wUYnaxVhSRA"
      },
      "source": [
        "#save model for later use\n",
        "from keras.models import load_model\n",
        "model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC2YSiozDixC"
      },
      "source": [
        "#load model \r\n",
        "model = load_model('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd0TBv7_hVh6"
      },
      "source": [
        "#plot\n",
        "traits_labels = df[[\"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]]\n",
        "\n",
        "\n",
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 10\n",
        "fig_size[1] = 8\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "\n",
        "traits_labels.sum(axis=0).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IsCBLJ-hWcw"
      },
      "source": [
        "# predictions = model.predict(np.expand_dims(X_test[500], 0))\n",
        "\n",
        "# print(tokenizer.sequences_to_texts([X_test[500]]))\n",
        "# print(y_test[500])\n",
        "# print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}